rm: cannot remove '*#*': No such file or directory
                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-3.142.dat -nsteps 100000 -x Colvars/traj_-3.142_-3.142.xtc

Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 

starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.482       14.120      400.0
                 (ns/day)    (hour/ns)
Performance:     1223.767        0.020

GROMACS reminds you: "Don't You Wish You Never Met Her, Dirty Blue Gene?" (Captain Beefheart)

 /home/ekempke/miniconda3/envs/emukit_env2/lib/python3.11/site-packages/paramz/transformations.py:111: RuntimeWarning:divide by zero encountered in log
Error occurred while running command: rm *#*
Command 'rm *#*' returned non-zero exit status 1.
# Integrating             - Simpson's rule + Real Space Grid Mini rm: cannot remove '*#*': No such file or directory
                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-3.142.dat -nsteps 100000 -x Colvars/traj_-3.142_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_-3.142.xtc to Colvars/#traj_-3.142_-3.142.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.845       13.961      400.0
                 (ns/day)    (hour/ns)
Performance:     1237.721        0.019

GROMACS reminds you: "Out Of Register Space (Ugh)" (Vi)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_0.000.dat -nsteps 100000 -x Colvars/traj_-3.142_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.554       14.139      400.0
                 (ns/day)    (hour/ns)
Performance:     1222.201        0.020

GROMACS reminds you: "Those people who think they know everything are a great annoyance to those of us who do." (Isaac Asimov)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_-3.142.dat -nsteps 100000 -x Colvars/traj_0.000_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.781       13.945      400.0
                 (ns/day)    (hour/ns)
Performance:     1239.148        0.019

GROMACS reminds you: "A computer once beat me at chess, but it was no match for me at kick boxing." (Emo Philips)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_0.000.dat -nsteps 100000 -x Colvars/traj_0.000_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.568       13.892      400.0
                 (ns/day)    (hour/ns)
Performance:     1243.893        0.019

GROMACS reminds you: "Bum Stikkie Di Bum Stikkie Di Bum Stikkie Di Bum" (R. Slijngaard)


# Integration error:        0.32

Error occurred while running command: rm *#*
Command 'rm *#*' returned non-zero exit status 1.
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-3.142.dat -nsteps 100000 -x Colvars/traj_-3.142_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_-3.142.xtc to Colvars/#traj_-3.142_-3.142.xtc.2#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.825       13.956      400.0
                 (ns/day)    (hour/ns)
Performance:     1238.154        0.019

GROMACS reminds you: "Jesus Not Only Saves, He Also Frequently Makes Backups." (Myron Bradshaw)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-1.047.dat -nsteps 100000 -x Colvars/traj_-3.142_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.093       14.023      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.239        0.019

GROMACS reminds you: "Bad times have a scientific value. These are occasions a good learner would not miss." (Ralph Waldo Emerson)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_1.047.dat -nsteps 100000 -x Colvars/traj_-3.142_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.640       13.910      400.0
                 (ns/day)    (hour/ns)
Performance:     1242.281        0.019

GROMACS reminds you: "Check Your Output" (P. Ahlstrom)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_-3.142.dat -nsteps 100000 -x Colvars/traj_-1.047_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.021       14.255      400.0
                 (ns/day)    (hour/ns)
Performance:     1212.191        0.020

GROMACS reminds you: "The final page is written in the books of history" (Bad Religion)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_-1.047.dat -nsteps 100000 -x Colvars/traj_-1.047_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.839       13.960      400.0
                 (ns/day)    (hour/ns)
Performance:     1237.857        0.019

GROMACS reminds you: "Load Up Your Rubber Bullets" (10 CC)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_1.047.dat -nsteps 100000 -x Colvars/traj_-1.047_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.524       14.131      400.0
                 (ns/day)    (hour/ns)
Performance:     1222.842        0.020

GROMACS reminds you: "I'm a Jerk" (F. Black)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_-3.142.dat -nsteps 100000 -x Colvars/traj_1.047_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       54.307       13.577      400.0
                 (ns/day)    (hour/ns)
Performance:     1272.765        0.019

GROMACS reminds you: "One Ripple At a Time" (Bianca's Smut Shack)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_-1.047.dat -nsteps 100000 -x Colvars/traj_1.047_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.634       13.908      400.0
                 (ns/day)    (hour/ns)
Performance:     1242.421        0.019

GROMACS reminds you: "I went to Venice and looked at the paintings of Canaletto to understand how he presented perspective, and it turned out it was an exponential law. If I had published this, maybe there would be a Karplus law in art theory as well as the Karplus equation in NMR" (Martin Karplus, Nobel lecture 2013)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_1.047.dat -nsteps 100000 -x Colvars/traj_1.047_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.628       14.157      400.0
                 (ns/day)    (hour/ns)
Performance:     1220.614        0.020

GROMACS reminds you: "I'd Be Water If I Could" (Red Hot Chili Peppers)


# Integration error:        0.87

# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-3.142.dat -nsteps 100000 -x Colvars/traj_-3.142_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_-3.142.xtc to Colvars/#traj_-3.142_-3.142.xtc.3#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.174       14.294      400.0
                 (ns/day)    (hour/ns)
Performance:     1208.941        0.020

GROMACS reminds you: "It is now quite lawful for a Catholic woman to avoid pregnancy by a resort to mathematics, though she is still forbidden to resort to physics and chemistry." (Henry Louis Mencken)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-1.571.dat -nsteps 100000 -x Colvars/traj_-3.142_-1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.668       14.167      400.0
                 (ns/day)    (hour/ns)
Performance:     1219.745        0.020

GROMACS reminds you: "Why would the backup server database get corrupted anyway?" (Stefan Fleischmann -- system administrator, physicist, optimist.)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_0.000.dat -nsteps 100000 -x Colvars/traj_-3.142_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_0.000.xtc to Colvars/#traj_-3.142_0.000.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.950       13.988      400.0
                 (ns/day)    (hour/ns)
Performance:     1235.399        0.019

GROMACS reminds you: "You always pass failure on the way to success." (Mickey Rooney)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_1.571.dat -nsteps 100000 -x Colvars/traj_-3.142_1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.048       14.012      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.238        0.019

GROMACS reminds you: "The future still looks good, and you've got time to rectify all the things that you should" (G. Harrison)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.571_-3.142.dat -nsteps 100000 -x Colvars/traj_-1.571_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.341       14.085      400.0
                 (ns/day)    (hour/ns)
Performance:     1226.814        0.020

GROMACS reminds you: "Science... never solves a problem without creating ten more." (George Bernard Shaw)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.571_-1.571.dat -nsteps 100000 -x Colvars/traj_-1.571_-1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.411       13.853      400.0
                 (ns/day)    (hour/ns)
Performance:     1247.415        0.019

GROMACS reminds you: "Teemu [Murtola] keeps beating our code, but that's fine because he's always right." (Berk Hess)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.571_0.000.dat -nsteps 100000 -x Colvars/traj_-1.571_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.820       13.955      400.0
                 (ns/day)    (hour/ns)
Performance:     1238.277        0.019

GROMACS reminds you: "In my opinion, we don't devote nearly enough scientific research to finding a cure for jerks." (Bill Watterson)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.571_1.571.dat -nsteps 100000 -x Colvars/traj_-1.571_1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.075       14.269      400.0
                 (ns/day)    (hour/ns)
Performance:     1211.051        0.020

GROMACS reminds you: "Here, kitty, kitty..." (Erwin Schroedinger)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_-3.142.dat -nsteps 100000 -x Colvars/traj_0.000_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_0.000_-3.142.xtc to Colvars/#traj_0.000_-3.142.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.044       14.261      400.0
                 (ns/day)    (hour/ns)
Performance:     1211.693        0.020

GROMACS reminds you: "Don't You Wish You Never Met Her, Dirty Blue Gene?" (Captain Beefheart)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_-1.571.dat -nsteps 100000 -x Colvars/traj_0.000_-1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.828       13.957      400.0
                 (ns/day)    (hour/ns)
Performance:     1238.085        0.019

GROMACS reminds you: "The most exciting phrase to hear in science, the one that heralds new discoveries, is not "Eureka" but "That's funny..."." (Isaac Asimov)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_0.000.dat -nsteps 100000 -x Colvars/traj_0.000_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_0.000_0.000.xtc to Colvars/#traj_0.000_0.000.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.457       13.864      400.0
                 (ns/day)    (hour/ns)
Performance:     1246.376        0.019

GROMACS reminds you: "I'm An Oakman" (Pulp Fiction)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_1.571.dat -nsteps 100000 -x Colvars/traj_0.000_1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.840       14.210      400.0
                 (ns/day)    (hour/ns)
Performance:     1216.056        0.020

GROMACS reminds you: "Martin [Karplus] had a green laser, Arieh [Warshel] had a red laser, I have a *blue* laser" (Michael Levitt, Nobel lecture 2013)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.571_-3.142.dat -nsteps 100000 -x Colvars/traj_1.571_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.890       14.223      400.0
                 (ns/day)    (hour/ns)
Performance:     1214.981        0.020

GROMACS reminds you: "Science is organized knowledge. Wisdom is organized life." (Immanuel Kant)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.571_-1.571.dat -nsteps 100000 -x Colvars/traj_1.571_-1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.850       14.213      400.0
                 (ns/day)    (hour/ns)
Performance:     1215.837        0.020

GROMACS reminds you: "What Kind Of Guru are You, Anyway ?" (F. Zappa)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.571_0.000.dat -nsteps 100000 -x Colvars/traj_1.571_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.553       14.138      400.0
                 (ns/day)    (hour/ns)
Performance:     1222.225        0.020

GROMACS reminds you: "Keep Your Shoes and Socks On, People" (F. Zappa)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.571_1.571.dat -nsteps 100000 -x Colvars/traj_1.571_1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.716       13.929      400.0
                 (ns/day)    (hour/ns)
Performance:     1240.595        0.019

GROMACS reminds you: "The physical chemists never use their eyes and are most lamentably lacking in chemical culture. It is essential to cast out from our midst, root and branch, this physical element and return to our laboratories." (Henry Edward Armstrong)


# Integration error:        2.41

# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-3.142.dat -nsteps 100000 -x Colvars/traj_-3.142_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_-3.142.xtc to Colvars/#traj_-3.142_-3.142.xtc.4#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.243       13.811      400.0
                 (ns/day)    (hour/ns)
Performance:     1251.213        0.019

GROMACS reminds you: "The loveliest theories are being overthrown by these damned experiments; it is no fun being a chemist any more." (Justus von Liebig, letter to J.J. Berzelius 1834)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-1.885.dat -nsteps 100000 -x Colvars/traj_-3.142_-1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.801       14.200      400.0
                 (ns/day)    (hour/ns)
Performance:     1216.894        0.020

GROMACS reminds you: "I have not failed. I've just found 10,000 ways that won't work" (Thomas Alva Edison)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-0.628.dat -nsteps 100000 -x Colvars/traj_-3.142_-0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.197       14.049      400.0
                 (ns/day)    (hour/ns)
Performance:     1229.969        0.020

GROMACS reminds you: "If at one time or another I have brushed a few colleagues the wrong way, I must apologize: I had not realized that they were covered with fur." (Edwin Chargaff)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_0.628.dat -nsteps 100000 -x Colvars/traj_-3.142_0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       53.381       13.345      400.0
                 (ns/day)    (hour/ns)
Performance:     1294.844        0.019

GROMACS reminds you: "There is no place like ~" (Anonymous)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_1.885.dat -nsteps 100000 -x Colvars/traj_-3.142_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.348       14.087      400.0
                 (ns/day)    (hour/ns)
Performance:     1226.679        0.020

GROMACS reminds you: "In the Meantime, Take Care of Yourself aaand Eachother" (J. Springer)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.885_-3.142.dat -nsteps 100000 -x Colvars/traj_-1.885_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.822       13.956      400.0
                 (ns/day)    (hour/ns)
Performance:     1238.230        0.019

GROMACS reminds you: "Youth is wasted on the young" (The Smashing Pumpkins)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.885_-1.885.dat -nsteps 100000 -x Colvars/traj_-1.885_-1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.749       13.937      400.0
                 (ns/day)    (hour/ns)
Performance:     1239.841        0.019

GROMACS reminds you: "I'll Match Your DNA" (Red Hot Chili Peppers)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.885_-0.628.dat -nsteps 100000 -x Colvars/traj_-1.885_-0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.541       13.885      400.0
                 (ns/day)    (hour/ns)
Performance:     1244.497        0.019

GROMACS reminds you: "Come on boys, Let's push it hard" (P.J. Harvey)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.885_0.628.dat -nsteps 100000 -x Colvars/traj_-1.885_0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.590       14.148      400.0
                 (ns/day)    (hour/ns)
Performance:     1221.415        0.020

GROMACS reminds you: "Oh My God ! It's the Funky Shit" (Beastie Boys)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.885_1.885.dat -nsteps 100000 -x Colvars/traj_-1.885_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.574       14.144      400.0
                 (ns/day)    (hour/ns)
Performance:     1221.771        0.020

GROMACS reminds you: "Safety lights are for dudes" (Ghostbusters 2016)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.628_-3.142.dat -nsteps 100000 -x Colvars/traj_-0.628_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.342       14.086      400.0
                 (ns/day)    (hour/ns)
Performance:     1226.797        0.020

GROMACS reminds you: "Home computers are being called upon to perform many new functions, including the consumption of homework formerly eaten by the dog." (Doug Larson)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.628_-1.885.dat -nsteps 100000 -x Colvars/traj_-0.628_-1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.074       14.019      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.660        0.019

GROMACS reminds you: "Screw a Lightbulb in your Head" (Gogol Bordello)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.628_-0.628.dat -nsteps 100000 -x Colvars/traj_-0.628_-0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.539       14.135      400.0
                 (ns/day)    (hour/ns)
Performance:     1222.536        0.020

GROMACS reminds you: "Put Me Inside SSC, Let's Test Superstring Theory, Oh Yoi Yoi Accelerate the Protons" (Gogol Bordello)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.628_0.628.dat -nsteps 100000 -x Colvars/traj_-0.628_0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.891       13.973      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.704        0.019

GROMACS reminds you: "C has the power of assembly language and the convenience of... assembly language." (Dennis Ritchie)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.628_1.885.dat -nsteps 100000 -x Colvars/traj_-0.628_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.157       14.039      400.0
                 (ns/day)    (hour/ns)
Performance:     1230.831        0.019

GROMACS reminds you: "In the End Science Comes Down to Praying" (P. v.d. Berg)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.628_-3.142.dat -nsteps 100000 -x Colvars/traj_0.628_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.131       14.033      400.0
                 (ns/day)    (hour/ns)
Performance:     1231.420        0.019

GROMACS reminds you: "I believe in miracles cause I'm one" (The Ramones)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.628_-1.885.dat -nsteps 100000 -x Colvars/traj_0.628_-1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.454       14.114      400.0
                 (ns/day)    (hour/ns)
Performance:     1224.358        0.020

GROMACS reminds you: "I love deadlines. I like the whooshing sound they make as they fly by." (Douglas Adams)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.628_-0.628.dat -nsteps 100000 -x Colvars/traj_0.628_-0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       53.283       13.321      400.0
                 (ns/day)    (hour/ns)
Performance:     1297.232        0.019

GROMACS reminds you: "They don't have half hours in the north" (Carl Caleman)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.628_0.628.dat -nsteps 100000 -x Colvars/traj_0.628_0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.964       13.991      400.0
                 (ns/day)    (hour/ns)
Performance:     1235.097        0.019

GROMACS reminds you: "I don't fear death because I don't fear anything I don't understand." (Hedy Lamarr)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.628_1.885.dat -nsteps 100000 -x Colvars/traj_0.628_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.662       14.165      400.0
                 (ns/day)    (hour/ns)
Performance:     1219.880        0.020

GROMACS reminds you: "A scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die and a new generation grows up that is familiar with it." (Max Planck)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.885_-3.142.dat -nsteps 100000 -x Colvars/traj_1.885_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.350       14.088      400.0
                 (ns/day)    (hour/ns)
Performance:     1226.623        0.020

GROMACS reminds you: "Culture eats strategy for breakfast" (Peter Drucker)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.885_-1.885.dat -nsteps 100000 -x Colvars/traj_1.885_-1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.255       14.314      400.0
                 (ns/day)    (hour/ns)
Performance:     1207.242        0.020

GROMACS reminds you: "Put Me Inside SSC, Let's Test Superstring Theory, Oh Yoi Yoi Accelerate the Protons" (Gogol Bordello)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.885_-0.628.dat -nsteps 100000 -x Colvars/traj_1.885_-0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.880       13.970      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.937        0.019

GROMACS reminds you: "A computer without COBOL and FORTRAN is like a piece of chocolate cake without ketchup or mustard." (Unix fortune program)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.885_0.628.dat -nsteps 100000 -x Colvars/traj_1.885_0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.034       14.008      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.551        0.019

GROMACS reminds you: "I Wrapped a Newspaper Round My Head" (F. Zappa)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.885_1.885.dat -nsteps 100000 -x Colvars/traj_1.885_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.054       14.264      400.0
                 (ns/day)    (hour/ns)
Performance:     1211.486        0.020

GROMACS reminds you: "Do you have mole problems? If so, call Avogadro at 602-1023." (Jay Leno)


# Integration error:        2.62

# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-3.142.dat -nsteps 100000 -x Colvars/traj_-3.142_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_-3.142.xtc to Colvars/#traj_-3.142_-3.142.xtc.5#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.140       14.035      400.0
                 (ns/day)    (hour/ns)
Performance:     1231.218        0.019

GROMACS reminds you: "Shake Yourself" (YES)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-2.094.dat -nsteps 100000 -x Colvars/traj_-3.142_-2.094.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.053       14.013      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.118        0.019

GROMACS reminds you: "I'd Like Monday Mornings Better If They Started Later" (Garfield)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-1.047.dat -nsteps 100000 -x Colvars/traj_-3.142_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_-1.047.xtc to Colvars/#traj_-3.142_-1.047.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.756       14.189      400.0
                 (ns/day)    (hour/ns)
Performance:     1217.841        0.020

GROMACS reminds you: "Water is just water" (Berk Hess)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_0.000.dat -nsteps 100000 -x Colvars/traj_-3.142_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_0.000.xtc to Colvars/#traj_-3.142_0.000.xtc.2#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.946       13.986      400.0
                 (ns/day)    (hour/ns)
Performance:     1235.495        0.019

GROMACS reminds you: "Royale With Cheese" (Pulp Fiction)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_1.047.dat -nsteps 100000 -x Colvars/traj_-3.142_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_1.047.xtc to Colvars/#traj_-3.142_1.047.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.801       13.950      400.0
                 (ns/day)    (hour/ns)
Performance:     1238.687        0.019

GROMACS reminds you: "You're Insignificant" (Tricky)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_2.094.dat -nsteps 100000 -x Colvars/traj_-3.142_2.094.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       53.740       13.435      400.0
                 (ns/day)    (hour/ns)
Performance:     1286.204        0.019

GROMACS reminds you: "Move Over Hogey Bear" (Urban Dance Squad)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.094_-3.142.dat -nsteps 100000 -x Colvars/traj_-2.094_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.363       14.091      400.0
                 (ns/day)    (hour/ns)
Performance:     1226.352        0.020

GROMACS reminds you: "Ease Myself Into the Body Bag" (P.J. Harvey)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.094_-2.094.dat -nsteps 100000 -x Colvars/traj_-2.094_-2.094.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       53.864       13.466      400.0
                 (ns/day)    (hour/ns)
Performance:     1283.236        0.019

GROMACS reminds you: "Wait a Minute, aren't You.... ? (gunshots) Yeah." (Bodycount)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.094_-1.047.dat -nsteps 100000 -x Colvars/traj_-2.094_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.550       13.887      400.0
                 (ns/day)    (hour/ns)
Performance:     1244.297        0.019

GROMACS reminds you: "I'd Be Water If I Could" (Red Hot Chili Peppers)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.094_0.000.dat -nsteps 100000 -x Colvars/traj_-2.094_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.280       14.070      400.0
                 (ns/day)    (hour/ns)
Performance:     1228.157        0.020

GROMACS reminds you: "If you don't know what you're doing, use a (M)BAR-based method" (Erik Lindahl)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.094_1.047.dat -nsteps 100000 -x Colvars/traj_-2.094_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.786       14.197      400.0
                 (ns/day)    (hour/ns)
Performance:     1217.204        0.020

GROMACS reminds you: "PHP is a minor evil perpetrated and created by incompetent amateurs, whereas Perl is a great and insidious evil, perpetrated by skilled but perverted professionals." (Jon Ribbens)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.094_2.094.dat -nsteps 100000 -x Colvars/traj_-2.094_2.094.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.338       14.335      400.0
                 (ns/day)    (hour/ns)
Performance:     1205.489        0.020

GROMACS reminds you: "gmx fellowship-writing -g grant_name -s protein_structure_involved -o output -m method_used -p list_of_pi" (Tanadet Pipatpolkai, while discussing new features for GROMACS)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_-3.142.dat -nsteps 100000 -x Colvars/traj_-1.047_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-1.047_-3.142.xtc to Colvars/#traj_-1.047_-3.142.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.042       13.761      400.0
                 (ns/day)    (hour/ns)
Performance:     1255.764        0.019

GROMACS reminds you: "Suzy is a headbanger, her mother is a geek" (The Ramones)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_-2.094.dat -nsteps 100000 -x Colvars/traj_-1.047_-2.094.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.635       13.909      400.0
                 (ns/day)    (hour/ns)
Performance:     1242.398        0.019

GROMACS reminds you: "Contemplating answers that could break my bonds." (Peter Hammill)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_-1.047.dat -nsteps 100000 -x Colvars/traj_-1.047_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-1.047_-1.047.xtc to Colvars/#traj_-1.047_-1.047.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.525       13.881      400.0
                 (ns/day)    (hour/ns)
Performance:     1244.862        0.019

GROMACS reminds you: "Where all think alike, no one thinks very much." (Walter Lippmann)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_0.000.dat -nsteps 100000 -x Colvars/traj_-1.047_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.098       14.025      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.139        0.019

GROMACS reminds you: "These Gromacs Guys Really Rock" (P.J. Meulenhoff)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_1.047.dat -nsteps 100000 -x Colvars/traj_-1.047_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-1.047_1.047.xtc to Colvars/#traj_-1.047_1.047.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.363       13.841      400.0
                 (ns/day)    (hour/ns)
Performance:     1248.483        0.019

GROMACS reminds you: "If You See Me Getting High, Knock Me Down" (Red Hot Chili Peppers)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_2.094.dat -nsteps 100000 -x Colvars/traj_-1.047_2.094.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.772       13.943      400.0
                 (ns/day)    (hour/ns)
Performance:     1239.330        0.019

GROMACS reminds you: "All Work and No Play Makes Jack a Dull Boy" (The Shining)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_-3.142.dat -nsteps 100000 -x Colvars/traj_0.000_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_0.000_-3.142.xtc to Colvars/#traj_0.000_-3.142.xtc.2#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.447       14.112      400.0
                 (ns/day)    (hour/ns)
Performance:     1224.509        0.020

GROMACS reminds you: "And after some more talk we agreed that the wisdom of rats had been grossly overrated, being in fact no greater than that of men" (Joseph Conrad)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_-2.094.dat -nsteps 100000 -x Colvars/traj_0.000_-2.094.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.070       14.018      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.744        0.019

GROMACS reminds you: "First off, I'd suggest printing out a copy of the GNU coding standards, and NOT read it. Burn them, it's a great symbolic gesture." (Linus Torvalds)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_-1.047.dat -nsteps 100000 -x Colvars/traj_0.000_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.146       14.287      400.0
                 (ns/day)    (hour/ns)
Performance:     1209.539        0.020

GROMACS reminds you: "How wonderful that we have met with a paradox. Now we have some hope of making progress." (Niels Bohr)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_0.000.dat -nsteps 100000 -x Colvars/traj_0.000_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_0.000_0.000.xtc to Colvars/#traj_0.000_0.000.xtc.2#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.052       14.013      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.156        0.019

GROMACS reminds you: "The soul? There's nothing but chemistry here" (Breaking Bad)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_1.047.dat -nsteps 100000 -x Colvars/traj_0.000_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.405       14.101      400.0
                 (ns/day)    (hour/ns)
Performance:     1225.440        0.020

GROMACS reminds you: "Courage is like - it's a habitus, a habit, a virtue: you get it by courageous acts. It's like you learn to swim by swimming. You learn courage by couraging." (Marie Daly)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_2.094.dat -nsteps 100000 -x Colvars/traj_0.000_2.094.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.192       14.298      400.0
                 (ns/day)    (hour/ns)
Performance:     1208.569        0.020

GROMACS reminds you: "You Got to Relate to It" (A.E. Torda)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_-3.142.dat -nsteps 100000 -x Colvars/traj_1.047_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_1.047_-3.142.xtc to Colvars/#traj_1.047_-3.142.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.620       14.155      400.0
                 (ns/day)    (hour/ns)
Performance:     1220.772        0.020

GROMACS reminds you: "I Am a Poor Lonesome Cowboy" (Lucky Luke)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_-2.094.dat -nsteps 100000 -x Colvars/traj_1.047_-2.094.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.139       14.035      400.0
                 (ns/day)    (hour/ns)
Performance:     1231.246        0.019

GROMACS reminds you: "The difficulty lies, not in the new ideas, but in escaping the old ones." (John Maynard Keynes)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_-1.047.dat -nsteps 100000 -x Colvars/traj_1.047_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_1.047_-1.047.xtc to Colvars/#traj_1.047_-1.047.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       54.005       13.501      400.0
                 (ns/day)    (hour/ns)
Performance:     1279.895        0.019

GROMACS reminds you: "Step On the Brakes" (2 Unlimited)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_0.000.dat -nsteps 100000 -x Colvars/traj_1.047_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.978       13.994      400.0
                 (ns/day)    (hour/ns)
Performance:     1234.786        0.019

GROMACS reminds you: "Jesus Not Only Saves, He Also Frequently Makes Backups." (Myron Bradshaw)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_1.047.dat -nsteps 100000 -x Colvars/traj_1.047_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_1.047_1.047.xtc to Colvars/#traj_1.047_1.047.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.996       13.999      400.0
                 (ns/day)    (hour/ns)
Performance:     1234.388        0.019

GROMACS reminds you: "Science, for me, gives a partial explanation for life. In so far as it goes, it is based on fact, experience and experiment." (Rosalind Franklin)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_2.094.dat -nsteps 100000 -x Colvars/traj_1.047_2.094.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.028       14.007      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.667        0.019

GROMACS reminds you: "Consistently separating words by spaces became a general custom about the tenth century A.D., and lasted until about 1957, when FORTRAN abandoned the practice." (Sun FORTRAN Reference Manual)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.094_-3.142.dat -nsteps 100000 -x Colvars/traj_2.094_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.125       14.281      400.0
                 (ns/day)    (hour/ns)
Performance:     1209.982        0.020

GROMACS reminds you: "Let us not get carried away with our ideas and take our models too seriously" (Nancy Swanson)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.094_-2.094.dat -nsteps 100000 -x Colvars/traj_2.094_-2.094.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.482       14.371      400.0
                 (ns/day)    (hour/ns)
Performance:     1202.471        0.020

GROMACS reminds you: "Hey, it's me - Pandora. Welcome to my new unboxing video!" (Anonymous)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.094_-1.047.dat -nsteps 100000 -x Colvars/traj_2.094_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.496       14.124      400.0
                 (ns/day)    (hour/ns)
Performance:     1223.455        0.020

GROMACS reminds you: "Sometimes Life is Obscene" (Black Crowes)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.094_0.000.dat -nsteps 100000 -x Colvars/traj_2.094_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.568       13.892      400.0
                 (ns/day)    (hour/ns)
Performance:     1243.877        0.019

GROMACS reminds you: "At school I had a teacher that didn't like me and I didn't like him. At the end of the year he decided to fail me. The ironic thing is that the topic was chemistry. I have the distinction of being the only Chemistry Laurate who failed the topic in high school." (Thomas Lindahl)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.094_1.047.dat -nsteps 100000 -x Colvars/traj_2.094_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.426       14.107      400.0
                 (ns/day)    (hour/ns)
Performance:     1224.969        0.020

GROMACS reminds you: "If at one time or another I have brushed a few colleagues the wrong way, I must apologize: I had not realized that they were covered with fur." (Edwin Chargaff)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.094_2.094.dat -nsteps 100000 -x Colvars/traj_2.094_2.094.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.741       14.185      400.0
                 (ns/day)    (hour/ns)
Performance:     1218.177        0.020

GROMACS reminds you: "Any one who considers arithmetical methods of producing random digits is, of course, in a state of sin." (John von Neumann)


# Integration error:        2.30

# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-3.142.dat -nsteps 100000 -x Colvars/traj_-3.142_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_-3.142.xtc to Colvars/#traj_-3.142_-3.142.xtc.6#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.076       14.019      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.612        0.019

GROMACS reminds you: "Shaken, not Stirred" (J. Bond)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-2.244.dat -nsteps 100000 -x Colvars/traj_-3.142_-2.244.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.339       14.085      400.0
                 (ns/day)    (hour/ns)
Performance:     1226.861        0.020

GROMACS reminds you: "For the first time we can now mechanically simulate the cognitive process. We can make studies in artificial intelligence. Beyond that, this mechanism can be used to assist humans in learning. As we are going to have more mature students in greater numbers as time goes on, this type of teaching will probably be increasingly important." (Sister Mary Kenneth Keller)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-1.346.dat -nsteps 100000 -x Colvars/traj_-3.142_-1.346.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.091       14.023      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.282        0.019

GROMACS reminds you: "There are only two hard things in computer science - cache invalidation, naming things and off-by-one errors." (Anonymous)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-0.449.dat -nsteps 100000 -x Colvars/traj_-3.142_-0.449.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.973       13.993      400.0
                 (ns/day)    (hour/ns)
Performance:     1234.891        0.019

GROMACS reminds you: "Step On the Brakes" (2 Unlimited)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_0.449.dat -nsteps 100000 -x Colvars/traj_-3.142_0.449.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.232       14.058      400.0
                 (ns/day)    (hour/ns)
Performance:     1229.210        0.020

GROMACS reminds you: "A university faculty is 500 egotists with a common parking problem." (Keith Sullivan)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_1.346.dat -nsteps 100000 -x Colvars/traj_-3.142_1.346.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.318       14.080      400.0
                 (ns/day)    (hour/ns)
Performance:     1227.323        0.020

GROMACS reminds you: "I Wrapped a Newspaper Round My Head" (F. Zappa)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_2.244.dat -nsteps 100000 -x Colvars/traj_-3.142_2.244.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       53.720       13.430      400.0
                 (ns/day)    (hour/ns)
Performance:     1286.679        0.019

GROMACS reminds you: "You're About to Hurt Somebody" (Jazzy Jeff)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.244_-3.142.dat -nsteps 100000 -x Colvars/traj_-2.244_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       53.937       13.484      400.0
                 (ns/day)    (hour/ns)
Performance:     1281.492        0.019

GROMACS reminds you: "If 10 years from now, when you are doing something quick and dirty, you suddenly visualize that I am looking over your shoulders and say to yourself: 'Dijkstra would not have liked this', well that would be enough immortality for me." (Edsger Dijkstra)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.244_-2.244.dat -nsteps 100000 -x Colvars/traj_-2.244_-2.244.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.072       14.018      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.705        0.019

GROMACS reminds you: "Computer system analysis is like child-rearing; you can do grievous damage, but you cannot ensure success." (Tom DeMarcho)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.244_-1.346.dat -nsteps 100000 -x Colvars/traj_-2.244_-1.346.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.885       13.971      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.831        0.019

GROMACS reminds you: "This is extremely unlikely." (Berk Hess)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.244_-0.449.dat -nsteps 100000 -x Colvars/traj_-2.244_-0.449.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.766       13.941      400.0
                 (ns/day)    (hour/ns)
Performance:     1239.482        0.019

GROMACS reminds you: "Naive you are if you believe life favours those who aren't naive." (Piet Hein)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.244_0.449.dat -nsteps 100000 -x Colvars/traj_-2.244_0.449.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.238       14.060      400.0
                 (ns/day)    (hour/ns)
Performance:     1229.062        0.020

GROMACS reminds you: "Never attribute to malice that which can be adequately explained by stupidity." (Robert Hanlon)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.244_1.346.dat -nsteps 100000 -x Colvars/traj_-2.244_1.346.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.895       13.974      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.600        0.019

GROMACS reminds you: "It's Bicycle Repair Man !" (Monty Python)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.244_2.244.dat -nsteps 100000 -x Colvars/traj_-2.244_2.244.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.217       14.054      400.0
                 (ns/day)    (hour/ns)
Performance:     1229.530        0.020

GROMACS reminds you: "Oh My God ! It's the Funky Shit" (Beastie Boys)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.346_-3.142.dat -nsteps 100000 -x Colvars/traj_-1.346_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.749       14.187      400.0
                 (ns/day)    (hour/ns)
Performance:     1217.998        0.020

GROMACS reminds you: "These Gromacs Guys Really Rock" (P.J. Meulenhoff)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.346_-2.244.dat -nsteps 100000 -x Colvars/traj_-1.346_-2.244.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.231       14.058      400.0
                 (ns/day)    (hour/ns)
Performance:     1229.222        0.020

GROMACS reminds you: "According to my computations we're overdue for a transformation." (Jackson Browne)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.346_-1.346.dat -nsteps 100000 -x Colvars/traj_-1.346_-1.346.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.997       13.999      400.0
                 (ns/day)    (hour/ns)
Performance:     1234.349        0.019

GROMACS reminds you: "A program that has not been tested does not work." (Bjarne Stroustrup)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.346_-0.449.dat -nsteps 100000 -x Colvars/traj_-1.346_-0.449.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.907       13.977      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.335        0.019

GROMACS reminds you: "Royale With Cheese" (Pulp Fiction)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.346_0.449.dat -nsteps 100000 -x Colvars/traj_-1.346_0.449.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.068       14.017      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.798        0.019

GROMACS reminds you: "FORTRAN, the infantile disorder, by now nearly 20 years old, is hopelessly inadequate for whatever computer application you have in mind today: it is now too clumsy, too risky, and too expensive to use." (Edsger Dijkstra, 1970)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.346_1.346.dat -nsteps 100000 -x Colvars/traj_-1.346_1.346.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.934       13.984      400.0
                 (ns/day)    (hour/ns)
Performance:     1235.748        0.019

GROMACS reminds you: "There is nothing new to be discovered in physics now. All that remains is more and more precise measurement." (Lord Kelvin, 1900)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.346_2.244.dat -nsteps 100000 -x Colvars/traj_-1.346_2.244.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.543       13.886      400.0
                 (ns/day)    (hour/ns)
Performance:     1244.458        0.019

GROMACS reminds you: ""Everything organic and natural is good" - ignoring the fact that organic natural substances include arsenic and poo and crocodiles. And everything chemical is bad, ignoring the fact that... everything is chemicals." (Tim Minchin)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.449_-3.142.dat -nsteps 100000 -x Colvars/traj_-0.449_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.027       14.007      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.700        0.019

GROMACS reminds you: "Computer science is no more about computers than astronomy is about telescopes" (Edsger Dijkstra)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.449_-2.244.dat -nsteps 100000 -x Colvars/traj_-0.449_-2.244.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.059       14.015      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.986        0.019

GROMACS reminds you: "Microbiology Lab - Staph Only." (Anonymous)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.449_-1.346.dat -nsteps 100000 -x Colvars/traj_-0.449_-1.346.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.335       14.084      400.0
                 (ns/day)    (hour/ns)
Performance:     1226.960        0.020

GROMACS reminds you: "Count the Bubbles In Your Hair" (The Breeders)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.449_-0.449.dat -nsteps 100000 -x Colvars/traj_-0.449_-0.449.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.478       14.120      400.0
                 (ns/day)    (hour/ns)
Performance:     1223.835        0.020

GROMACS reminds you: "You still have to climb to the shoulders of the giants" (Vedran Miletic)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.449_0.449.dat -nsteps 100000 -x Colvars/traj_-0.449_0.449.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.158       14.040      400.0
                 (ns/day)    (hour/ns)
Performance:     1230.815        0.019

GROMACS reminds you: "I Had So Many Problem, and Then I Got Me a Walkman" (F. Black)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.449_1.346.dat -nsteps 100000 -x Colvars/traj_-0.449_1.346.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.014       14.004      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.981        0.019

GROMACS reminds you: "We'll celebrate a woman for anything, as long as it's not her talent." (Colleen McCullough)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.449_2.244.dat -nsteps 100000 -x Colvars/traj_-0.449_2.244.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.816       13.954      400.0
                 (ns/day)    (hour/ns)
Performance:     1238.361        0.019

GROMACS reminds you: "Youth is wasted on the young" (The Smashing Pumpkins)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.449_-3.142.dat -nsteps 100000 -x Colvars/traj_0.449_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.292       14.073      400.0
                 (ns/day)    (hour/ns)
Performance:     1227.892        0.020

GROMACS reminds you: "If You're So Special Why aren't You Dead ?" (The Breeders)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.449_-2.244.dat -nsteps 100000 -x Colvars/traj_0.449_-2.244.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.854       13.964      400.0
                 (ns/day)    (hour/ns)
Performance:     1237.522        0.019

GROMACS reminds you: "This simulation is not as the former." (Malvolio, Act II, scene V of Shaphespeare's Twelfth Night)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.449_-1.346.dat -nsteps 100000 -x Colvars/traj_0.449_-1.346.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.786       13.947      400.0
                 (ns/day)    (hour/ns)
Performance:     1239.027        0.019

GROMACS reminds you: "If you have any trouble sounding condescending, find a UNIX user to show you how it's done." (Scott Adams, Dilbert Cartoonist)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.449_-0.449.dat -nsteps 100000 -x Colvars/traj_0.449_-0.449.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.260       14.065      400.0
                 (ns/day)    (hour/ns)
Performance:     1228.580        0.020

GROMACS reminds you: "Some People Say Not to Worry About the Air" (The Talking Heads)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.449_0.449.dat -nsteps 100000 -x Colvars/traj_0.449_0.449.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       53.569       13.392      400.0
                 (ns/day)    (hour/ns)
Performance:     1290.298        0.019

GROMACS reminds you: "I've basically become a vegetarian since the only meat I'm eating is from animals I've killed myself" (Mark Zuckerberg)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.449_1.346.dat -nsteps 100000 -x Colvars/traj_0.449_1.346.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.968       13.992      400.0
                 (ns/day)    (hour/ns)
Performance:     1234.992        0.019

GROMACS reminds you: "Fifty years of programming language research, and we end up with C++???" (Richard O'Keefe)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.449_2.244.dat -nsteps 100000 -x Colvars/traj_0.449_2.244.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.624       14.156      400.0
                 (ns/day)    (hour/ns)
Performance:     1220.700        0.020

GROMACS reminds you: "That Was Really Cool" (Butthead)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.346_-3.142.dat -nsteps 100000 -x Colvars/traj_1.346_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.921       13.980      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.041        0.019

GROMACS reminds you: "It's So Fast It's Slow" (F. Black)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.346_-2.244.dat -nsteps 100000 -x Colvars/traj_1.346_-2.244.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.713       14.178      400.0
                 (ns/day)    (hour/ns)
Performance:     1218.768        0.020

GROMACS reminds you: "I Quit My Job Blowing Leaves" (Beck)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.346_-1.346.dat -nsteps 100000 -x Colvars/traj_1.346_-1.346.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.711       14.178      400.0
                 (ns/day)    (hour/ns)
Performance:     1218.815        0.020

GROMACS reminds you: "These Gromacs Guys Really Rock" (P.J. Meulenhoff)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.346_-0.449.dat -nsteps 100000 -x Colvars/traj_1.346_-0.449.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.608       14.152      400.0
                 (ns/day)    (hour/ns)
Performance:     1221.039        0.020

GROMACS reminds you: "It's Not Your Fault" (Pulp Fiction)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.346_0.449.dat -nsteps 100000 -x Colvars/traj_1.346_0.449.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.574       14.143      400.0
                 (ns/day)    (hour/ns)
Performance:     1221.779        0.020

GROMACS reminds you: "Confirmed" (Star Trek)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.346_1.346.dat -nsteps 100000 -x Colvars/traj_1.346_1.346.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.946       13.987      400.0
                 (ns/day)    (hour/ns)
Performance:     1235.478        0.019

GROMACS reminds you: "Is This the Right Room for an Argument ?" (Monty Python)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.346_2.244.dat -nsteps 100000 -x Colvars/traj_1.346_2.244.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.022       14.006      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.811        0.019

GROMACS reminds you: "It Just Tastes Better" (Burger King)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.244_-3.142.dat -nsteps 100000 -x Colvars/traj_2.244_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.755       14.189      400.0
                 (ns/day)    (hour/ns)
Performance:     1217.864        0.020

GROMACS reminds you: "AH ....Satisfaction" (IRIX imapd)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.244_-2.244.dat -nsteps 100000 -x Colvars/traj_2.244_-2.244.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.993       13.998      400.0
                 (ns/day)    (hour/ns)
Performance:     1234.442        0.019

GROMACS reminds you: "A robot will be truly autonomous when you instruct it to go to work and it decides to go to the beach instead." (Brad Templeton)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.244_-1.346.dat -nsteps 100000 -x Colvars/traj_2.244_-1.346.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.990       13.998      400.0
                 (ns/day)    (hour/ns)
Performance:     1234.515        0.019

GROMACS reminds you: "Rockets are cool. There's no getting around that." (Elon Musk)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.244_-0.449.dat -nsteps 100000 -x Colvars/traj_2.244_-0.449.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.698       13.924      400.0
                 (ns/day)    (hour/ns)
Performance:     1240.991        0.019

GROMACS reminds you: "Nada e organico, e tudo programado" (Pitty)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.244_0.449.dat -nsteps 100000 -x Colvars/traj_2.244_0.449.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.306       14.077      400.0
                 (ns/day)    (hour/ns)
Performance:     1227.576        0.020

GROMACS reminds you: "I'm a Jerk" (F. Black)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.244_1.346.dat -nsteps 100000 -x Colvars/traj_2.244_1.346.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.031       14.008      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.616        0.019

GROMACS reminds you: "Science is a wonderful thing if one does not have to earn one's living at it." (Albert Einstein)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.244_2.244.dat -nsteps 100000 -x Colvars/traj_2.244_2.244.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.049       14.012      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.206        0.019

GROMACS reminds you: "Carbohydrates is all they groove" (Frank Zappa)


# Integration error:        2.16

# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-3.142.dat -nsteps 100000 -x Colvars/traj_-3.142_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_-3.142.xtc to Colvars/#traj_-3.142_-3.142.xtc.7#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       53.407       13.352      400.0
                 (ns/day)    (hour/ns)
Performance:     1294.215        0.019

GROMACS reminds you: "Marie, you're looking more radiant every day!" (Pierre Curie)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-2.356.dat -nsteps 100000 -x Colvars/traj_-3.142_-2.356.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.268       14.317      400.0
                 (ns/day)    (hour/ns)
Performance:     1206.958        0.020

GROMACS reminds you: "There are three types of people: Those who see, those who see when they are shown, and those who do not see." (Leonardo da Vinci)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-1.571.dat -nsteps 100000 -x Colvars/traj_-3.142_-1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_-1.571.xtc to Colvars/#traj_-3.142_-1.571.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.359       14.090      400.0
                 (ns/day)    (hour/ns)
Performance:     1226.436        0.020

GROMACS reminds you: "We must be clear that when it comes to atoms, language can be used only as in poetry. " (Niels Bohr)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-0.785.dat -nsteps 100000 -x Colvars/traj_-3.142_-0.785.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.789       13.947      400.0
                 (ns/day)    (hour/ns)
Performance:     1238.963        0.019

GROMACS reminds you: "Just a Minute While I Reinvent Myself" (Red Hot Chili Peppers)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_0.000.dat -nsteps 100000 -x Colvars/traj_-3.142_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_0.000.xtc to Colvars/#traj_-3.142_0.000.xtc.3#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.625       13.906      400.0
                 (ns/day)    (hour/ns)
Performance:     1242.612        0.019

GROMACS reminds you: "It's just the way this stuff is done" (Built to Spill)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_0.785.dat -nsteps 100000 -x Colvars/traj_-3.142_0.785.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.236       14.309      400.0
                 (ns/day)    (hour/ns)
Performance:     1207.647        0.020

GROMACS reminds you: "If you're doing I/O, you're doing it wrong!" (Cannada "Drew" Lewis)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_1.571.dat -nsteps 100000 -x Colvars/traj_-3.142_1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_1.571.xtc to Colvars/#traj_-3.142_1.571.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.699       13.925      400.0
                 (ns/day)    (hour/ns)
Performance:     1240.968        0.019

GROMACS reminds you: "If I could remember the names of all these particles, I'd be a botanist." (Albert Einstein)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_2.356.dat -nsteps 100000 -x Colvars/traj_-3.142_2.356.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.992       14.248      400.0
                 (ns/day)    (hour/ns)
Performance:     1212.802        0.020

GROMACS reminds you: "That Was Cool" (Beavis and Butthead)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.356_-3.142.dat -nsteps 100000 -x Colvars/traj_-2.356_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.138       14.035      400.0
                 (ns/day)    (hour/ns)
Performance:     1231.260        0.019

GROMACS reminds you: "Meet Me At the Coffee Shop" (Red Hot Chili Peppers)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.356_-2.356.dat -nsteps 100000 -x Colvars/traj_-2.356_-2.356.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.564       14.391      400.0
                 (ns/day)    (hour/ns)
Performance:     1200.763        0.020

GROMACS reminds you: "Therefore, things must be learned only to be unlearned again or, more likely, to be corrected." (Richard Feynman)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.356_-1.571.dat -nsteps 100000 -x Colvars/traj_-2.356_-1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.694       14.174      400.0
                 (ns/day)    (hour/ns)
Performance:     1219.183        0.020

GROMACS reminds you: "Sir, spare your threats: The bug which you would fright me with I seek." (Hermione, Act III, scene II of Shakespeare's Winter's Tale)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.356_-0.785.dat -nsteps 100000 -x Colvars/traj_-2.356_-0.785.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.030       14.258      400.0
                 (ns/day)    (hour/ns)
Performance:     1211.999        0.020

GROMACS reminds you: "Life need not be easy, provided only that it is not empty." (Lise Meitner)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.356_0.000.dat -nsteps 100000 -x Colvars/traj_-2.356_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.925       13.981      400.0
                 (ns/day)    (hour/ns)
Performance:     1235.943        0.019

GROMACS reminds you: "There are only two kinds of programming languages: those people always bitch about and those nobody uses." (Bjarne Stroustrup)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.356_0.785.dat -nsteps 100000 -x Colvars/traj_-2.356_0.785.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.191       14.048      400.0
                 (ns/day)    (hour/ns)
Performance:     1230.097        0.020

GROMACS reminds you: "All approaches at a higher level are suspect until confirmed at the molecular level." (Francis Crick)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.356_1.571.dat -nsteps 100000 -x Colvars/traj_-2.356_1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.892       13.973      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.676        0.019

GROMACS reminds you: "Or (horrors!) use Berendsen!" (Justin Lemkul)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.356_2.356.dat -nsteps 100000 -x Colvars/traj_-2.356_2.356.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.951       13.988      400.0
                 (ns/day)    (hour/ns)
Performance:     1235.382        0.019

GROMACS reminds you: "Way to Go Dude" (Beavis and Butthead)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.571_-3.142.dat -nsteps 100000 -x Colvars/traj_-1.571_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-1.571_-3.142.xtc to Colvars/#traj_-1.571_-3.142.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.294       14.323      400.0
                 (ns/day)    (hour/ns)
Performance:     1206.426        0.020

GROMACS reminds you: "Wicky-wicky Wa-wild West" (Will Smith)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.571_-2.356.dat -nsteps 100000 -x Colvars/traj_-1.571_-2.356.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.896       13.974      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.579        0.019

GROMACS reminds you: "The Candlelight Was Just Right" (Beastie Boys)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.571_-1.571.dat -nsteps 100000 -x Colvars/traj_-1.571_-1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-1.571_-1.571.xtc to Colvars/#traj_-1.571_-1.571.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.664       14.166      400.0
                 (ns/day)    (hour/ns)
Performance:     1219.834        0.020

GROMACS reminds you: "She Says She Can't Go Home Without a Chaperone" (E. Costello)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.571_-0.785.dat -nsteps 100000 -x Colvars/traj_-1.571_-0.785.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.076       14.269      400.0
                 (ns/day)    (hour/ns)
Performance:     1211.020        0.020

GROMACS reminds you: "It has been discovered that C++ provides a remarkable facility for concealing the trivial details of a program - such as where its bugs are." (David Keppel)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.571_0.000.dat -nsteps 100000 -x Colvars/traj_-1.571_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-1.571_0.000.xtc to Colvars/#traj_-1.571_0.000.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.759       13.940      400.0
                 (ns/day)    (hour/ns)
Performance:     1239.624        0.019

GROMACS reminds you: "The biggest lie in science is 'data and code available upon request'" (Michael Eisen)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.571_0.785.dat -nsteps 100000 -x Colvars/traj_-1.571_0.785.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.018       14.005      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.896        0.019

GROMACS reminds you: "With a Little Penknife" (Nick Cave)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.571_1.571.dat -nsteps 100000 -x Colvars/traj_-1.571_1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-1.571_1.571.xtc to Colvars/#traj_-1.571_1.571.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.787       13.947      400.0
                 (ns/day)    (hour/ns)
Performance:     1238.996        0.019

GROMACS reminds you: "Throughout my academic career, I'd given some pretty good talks. But being considered the best speaker in the computer science department is like being known as the tallest of the Seven Dwarfs." (Randy Pausch)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.571_2.356.dat -nsteps 100000 -x Colvars/traj_-1.571_2.356.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.219       14.055      400.0
                 (ns/day)    (hour/ns)
Performance:     1229.483        0.020

GROMACS reminds you: "Academe, n.: An ancient school where morality and philosophy were taught. Academy, n.: A modern school where football is taught." (Ambrose Bierce)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.785_-3.142.dat -nsteps 100000 -x Colvars/traj_-0.785_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.277       13.819      400.0
                 (ns/day)    (hour/ns)
Performance:     1250.429        0.019

GROMACS reminds you: "There's No Room For the Weak" (Joy Division)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.785_-2.356.dat -nsteps 100000 -x Colvars/traj_-0.785_-2.356.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.329       14.082      400.0
                 (ns/day)    (hour/ns)
Performance:     1227.092        0.020

GROMACS reminds you: "The easiest way to scale well is to have bad single-core performance" (Blind Freddie)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.785_-1.571.dat -nsteps 100000 -x Colvars/traj_-0.785_-1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.417       14.104      400.0
                 (ns/day)    (hour/ns)
Performance:     1225.172        0.020

GROMACS reminds you: "Science, my lad, is made up of mistakes, but they are mistakes which it is useful to make, because they lead little by little to the truth." (Jules Verne)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.785_-0.785.dat -nsteps 100000 -x Colvars/traj_-0.785_-0.785.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.918       13.979      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.110        0.019

GROMACS reminds you: "Proceed, With Fingers Crossed" (TeX)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.785_0.000.dat -nsteps 100000 -x Colvars/traj_-0.785_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.235       14.059      400.0
                 (ns/day)    (hour/ns)
Performance:     1229.124        0.020

GROMACS reminds you: "Every Sperm is Sacred" (Monty Python)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.785_0.785.dat -nsteps 100000 -x Colvars/traj_-0.785_0.785.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.844       13.961      400.0
                 (ns/day)    (hour/ns)
Performance:     1237.734        0.019

GROMACS reminds you: "Nobody ever complained a seminar was too easy to understand." (Ken Dill)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.785_1.571.dat -nsteps 100000 -x Colvars/traj_-0.785_1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       52.787       13.197      400.0
                 (ns/day)    (hour/ns)
Performance:     1309.423        0.018

GROMACS reminds you: "We are continually faced by great opportunities brilliantly disguised as insoluble problems." (Lee Iacocca)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.785_2.356.dat -nsteps 100000 -x Colvars/traj_-0.785_2.356.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.465       14.116      400.0
                 (ns/day)    (hour/ns)
Performance:     1224.127        0.020

GROMACS reminds you: "I Am Testing Your Grey Matter" (Red Hot Chili Peppers)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_-3.142.dat -nsteps 100000 -x Colvars/traj_0.000_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_0.000_-3.142.xtc to Colvars/#traj_0.000_-3.142.xtc.3#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.051       14.013      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.163        0.019

GROMACS reminds you: "Can't You Make This Thing Go Faster ?" (Black Crowes)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_-2.356.dat -nsteps 100000 -x Colvars/traj_0.000_-2.356.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.782       13.946      400.0
                 (ns/day)    (hour/ns)
Performance:     1239.112        0.019

GROMACS reminds you: "I am rarely happier than when spending an entire day programming my computer to perform automatically a task that it would otherwise take me a good ten seconds to do by hand." (Douglas Adams)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_-1.571.dat -nsteps 100000 -x Colvars/traj_0.000_-1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_0.000_-1.571.xtc to Colvars/#traj_0.000_-1.571.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.839       13.960      400.0
                 (ns/day)    (hour/ns)
Performance:     1237.850        0.019

GROMACS reminds you: "Prior to 1965 there were none, and after 1965 there was a nun." (Sister Mary Kenneth Keller regarding women with PhDs in computer science)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_-0.785.dat -nsteps 100000 -x Colvars/traj_0.000_-0.785.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.563       14.141      400.0
                 (ns/day)    (hour/ns)
Performance:     1222.010        0.020

GROMACS reminds you: "Science Won't Change You" (The Talking Heads)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_0.000.dat -nsteps 100000 -x Colvars/traj_0.000_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_0.000_0.000.xtc to Colvars/#traj_0.000_0.000.xtc.3#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.528       13.882      400.0
                 (ns/day)    (hour/ns)
Performance:     1244.787        0.019

GROMACS reminds you: "Considering the current sad state of our computer programs, software development is clearly still a black art, and cannot yet be called an engineering discipline." (William Jefferson Clinton)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_0.785.dat -nsteps 100000 -x Colvars/traj_0.000_0.785.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.035       14.009      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.525        0.019

GROMACS reminds you: "Uh-oh" (Tinky Winky)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_1.571.dat -nsteps 100000 -x Colvars/traj_0.000_1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_0.000_1.571.xtc to Colvars/#traj_0.000_1.571.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.847       13.962      400.0
                 (ns/day)    (hour/ns)
Performance:     1237.669        0.019

GROMACS reminds you: "The Path Of the Righteous Man is Beset On All Sides With the Iniquities Of the Selfish and the Tyranny Of Evil Men." (Pulp Fiction)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_2.356.dat -nsteps 100000 -x Colvars/traj_0.000_2.356.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.649       14.162      400.0
                 (ns/day)    (hour/ns)
Performance:     1220.153        0.020

GROMACS reminds you: "There's no kill like overkill, right?" (Erik Lindahl)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.785_-3.142.dat -nsteps 100000 -x Colvars/traj_0.785_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.754       14.189      400.0
                 (ns/day)    (hour/ns)
Performance:     1217.891        0.020

GROMACS reminds you: "Three Little Fonzies" (Pulp Fiction)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.785_-2.356.dat -nsteps 100000 -x Colvars/traj_0.785_-2.356.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.526       13.882      400.0
                 (ns/day)    (hour/ns)
Performance:     1244.823        0.019

GROMACS reminds you: "We're Gonna Hit You Harder" (Scoter)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.785_-1.571.dat -nsteps 100000 -x Colvars/traj_0.785_-1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.140       14.035      400.0
                 (ns/day)    (hour/ns)
Performance:     1231.218        0.019

GROMACS reminds you: "Cowardly refusing to create an empty archive" (GNU tar)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.785_-0.785.dat -nsteps 100000 -x Colvars/traj_0.785_-0.785.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.380       14.095      400.0
                 (ns/day)    (hour/ns)
Performance:     1225.983        0.020

GROMACS reminds you: "Don't Eat That Yellow Snow" (F. Zappa)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.785_0.000.dat -nsteps 100000 -x Colvars/traj_0.785_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.838       13.960      400.0
                 (ns/day)    (hour/ns)
Performance:     1237.874        0.019

GROMACS reminds you: "If I Wanted You to Understand This, I Would Explain it Better" (J. Cruijff)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.785_0.785.dat -nsteps 100000 -x Colvars/traj_0.785_0.785.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.820       14.205      400.0
                 (ns/day)    (hour/ns)
Performance:     1216.473        0.020

GROMACS reminds you: "First off, I'd suggest printing out a copy of the GNU coding standards, and NOT read it. Burn them, it's a great symbolic gesture." (Linus Torvalds)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.785_1.571.dat -nsteps 100000 -x Colvars/traj_0.785_1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.255       14.064      400.0
                 (ns/day)    (hour/ns)
Performance:     1228.693        0.020

GROMACS reminds you: "I ought to warn you, I have no faith" (Jane Eyre in Jane Eyre by Charlotte Bronte)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.785_2.356.dat -nsteps 100000 -x Colvars/traj_0.785_2.356.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.884       13.971      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.856        0.019

GROMACS reminds you: "Oh My God ! It's the Funky Shit" (Beastie Boys)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.571_-3.142.dat -nsteps 100000 -x Colvars/traj_1.571_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_1.571_-3.142.xtc to Colvars/#traj_1.571_-3.142.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.596       13.899      400.0
                 (ns/day)    (hour/ns)
Performance:     1243.253        0.019

GROMACS reminds you: "In a talk you have a choice: You can make one point or no points." (Paul Sigler)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.571_-2.356.dat -nsteps 100000 -x Colvars/traj_1.571_-2.356.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       54.927       13.732      400.0
                 (ns/day)    (hour/ns)
Performance:     1258.412        0.019

GROMACS reminds you: "It is now quite lawful for a Catholic woman to avoid pregnancy by a resort to mathematics, though she is still forbidden to resort to physics and chemistry." (Henry Louis Mencken)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.571_-1.571.dat -nsteps 100000 -x Colvars/traj_1.571_-1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_1.571_-1.571.xtc to Colvars/#traj_1.571_-1.571.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.334       14.083      400.0
                 (ns/day)    (hour/ns)
Performance:     1226.981        0.020

GROMACS reminds you: "To survive science you have to become science." (Gerrit Groenhof)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.571_-0.785.dat -nsteps 100000 -x Colvars/traj_1.571_-0.785.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.852       13.963      400.0
                 (ns/day)    (hour/ns)
Performance:     1237.562        0.019

GROMACS reminds you: "I always think there is something foreign about jolly phrases at breakfast." (Mr. Carson in Downtown Abbey)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.571_0.000.dat -nsteps 100000 -x Colvars/traj_1.571_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_1.571_0.000.xtc to Colvars/#traj_1.571_0.000.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.579       14.145      400.0
                 (ns/day)    (hour/ns)
Performance:     1221.669        0.020

GROMACS reminds you: "Science, my lad, is made up of mistakes, but they are mistakes which it is useful to make, because they lead little by little to the truth." (Jules Verne)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.571_0.785.dat -nsteps 100000 -x Colvars/traj_1.571_0.785.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.973       13.993      400.0
                 (ns/day)    (hour/ns)
Performance:     1234.884        0.019

GROMACS reminds you: "I admired Bohr very much. We had long talks together, long talks in which Bohr did practically all the talking." (Paul Dirac)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.571_1.571.dat -nsteps 100000 -x Colvars/traj_1.571_1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_1.571_1.571.xtc to Colvars/#traj_1.571_1.571.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.579       14.145      400.0
                 (ns/day)    (hour/ns)
Performance:     1221.662        0.020

GROMACS reminds you: "Facts are stubborn things, but statistics are more pliable." (Laurence Peter)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.571_2.356.dat -nsteps 100000 -x Colvars/traj_1.571_2.356.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.616       14.154      400.0
                 (ns/day)    (hour/ns)
Performance:     1220.866        0.020

GROMACS reminds you: "This really is a pretty scene, could you ask your kid to smile please?" (Joe Jackson)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.356_-3.142.dat -nsteps 100000 -x Colvars/traj_2.356_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.305       13.826      400.0
                 (ns/day)    (hour/ns)
Performance:     1249.806        0.019

GROMACS reminds you: "If it's all right with Dirac, it's all right with me." (Enrico Fermi, on being told that there was experimental evidence He-3 nuclei obey Fermi-Dirac statistics.)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.356_-2.356.dat -nsteps 100000 -x Colvars/traj_2.356_-2.356.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.520       14.130      400.0
                 (ns/day)    (hour/ns)
Performance:     1222.930        0.020

GROMACS reminds you: "Mathematics is no more computation than typing is literature." (John Allen Paulos)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.356_-1.571.dat -nsteps 100000 -x Colvars/traj_2.356_-1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.160       14.040      400.0
                 (ns/day)    (hour/ns)
Performance:     1230.786        0.019

GROMACS reminds you: "gmx fellowship-writing -g grant_name -s protein_structure_involved -o output -m method_used -p list_of_pi" (Tanadet Pipatpolkai, while discussing new features for GROMACS)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.356_-0.785.dat -nsteps 100000 -x Colvars/traj_2.356_-0.785.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.636       14.159      400.0
                 (ns/day)    (hour/ns)
Performance:     1220.428        0.020

GROMACS reminds you: "It's hard to ignore 12 orders of magnitude" (John Urbanic)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.356_0.000.dat -nsteps 100000 -x Colvars/traj_2.356_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       52.785       13.196      400.0
                 (ns/day)    (hour/ns)
Performance:     1309.461        0.018

GROMACS reminds you: "A ship in port is safe, but that is not what ships are for. Sail out to sea and do new things." (Grace Hopper, developer of COBOL)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.356_0.785.dat -nsteps 100000 -x Colvars/traj_2.356_0.785.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.073       14.018      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.677        0.019

GROMACS reminds you: "I'm a Jerk" (F. Black)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.356_1.571.dat -nsteps 100000 -x Colvars/traj_2.356_1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.162       14.290      400.0
                 (ns/day)    (hour/ns)
Performance:     1209.212        0.020

GROMACS reminds you: "The Internet?  We are not interested in it." (Bill Gates, 1993)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.356_2.356.dat -nsteps 100000 -x Colvars/traj_2.356_2.356.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.763       13.941      400.0
                 (ns/day)    (hour/ns)
Performance:     1239.536        0.019

GROMACS reminds you: "The time for theory is over" (J. Hajdu)


# Integration error:        2.64

# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-3.142.dat -nsteps 100000 -x Colvars/traj_-3.142_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_-3.142.xtc to Colvars/#traj_-3.142_-3.142.xtc.8#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.478       14.119      400.0
                 (ns/day)    (hour/ns)
Performance:     1223.856        0.020

GROMACS reminds you: "It's Time to Move On" (F. Black)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-2.443.dat -nsteps 100000 -x Colvars/traj_-3.142_-2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.283       14.321      400.0
                 (ns/day)    (hour/ns)
Performance:     1206.638        0.020

GROMACS reminds you: "It's more useful when you know what you're doing." (Artem Zhmurov)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-1.745.dat -nsteps 100000 -x Colvars/traj_-3.142_-1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.610       13.903      400.0
                 (ns/day)    (hour/ns)
Performance:     1242.944        0.019

GROMACS reminds you: "Science progresses best when observations force us to alter our preconceptions." (Vera Rubin)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-1.047.dat -nsteps 100000 -x Colvars/traj_-3.142_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_-1.047.xtc to Colvars/#traj_-3.142_-1.047.xtc.2#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.916       13.979      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.144        0.019

GROMACS reminds you: "If you don't know what you're doing, use a (M)BAR-based method" (Erik Lindahl)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-0.349.dat -nsteps 100000 -x Colvars/traj_-3.142_-0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.930       14.233      400.0
                 (ns/day)    (hour/ns)
Performance:     1214.127        0.020

GROMACS reminds you: "C is not a high-level language." (Brian Kernighan, C author)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_0.349.dat -nsteps 100000 -x Colvars/traj_-3.142_0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.102       14.275      400.0
                 (ns/day)    (hour/ns)
Performance:     1210.481        0.020

GROMACS reminds you: "Dont bring an anecdote to a data fight." (Molly Hodgdon)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_1.047.dat -nsteps 100000 -x Colvars/traj_-3.142_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_1.047.xtc to Colvars/#traj_-3.142_1.047.xtc.2#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.374       14.094      400.0
                 (ns/day)    (hour/ns)
Performance:     1226.099        0.020

GROMACS reminds you: "Enthusiasm is the mother of effort, and without it nothing great was ever achieved." (Ralph Waldo Emerson)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_1.745.dat -nsteps 100000 -x Colvars/traj_-3.142_1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.681       13.920      400.0
                 (ns/day)    (hour/ns)
Performance:     1241.358        0.019

GROMACS reminds you: "I'd be Safe and Warm if I was in L.A." (The Mamas and the Papas)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_2.443.dat -nsteps 100000 -x Colvars/traj_-3.142_2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       53.387       13.347      400.0
                 (ns/day)    (hour/ns)
Performance:     1294.706        0.019

GROMACS reminds you: "My Head Goes Pop Pop Pop Pop Pop" (F. Black)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.443_-3.142.dat -nsteps 100000 -x Colvars/traj_-2.443_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.824       13.956      400.0
                 (ns/day)    (hour/ns)
Performance:     1238.193        0.019

GROMACS reminds you: "We mathematicians are all a bit crazy." (Lev Landau)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.443_-2.443.dat -nsteps 100000 -x Colvars/traj_-2.443_-2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.334       14.083      400.0
                 (ns/day)    (hour/ns)
Performance:     1226.982        0.020

GROMACS reminds you: "It's more useful when you know what you're doing." (Artem Zhmurov)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.443_-1.745.dat -nsteps 100000 -x Colvars/traj_-2.443_-1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.230       14.058      400.0
                 (ns/day)    (hour/ns)
Performance:     1229.244        0.020

GROMACS reminds you: "I couldn't give a shit about ribosomes." (Bjrn Forsberg, presenting his thesis, including two papers on ribosomes)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.443_-1.047.dat -nsteps 100000 -x Colvars/traj_-2.443_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.207       14.052      400.0
                 (ns/day)    (hour/ns)
Performance:     1229.748        0.020

GROMACS reminds you: "I'd Be Water If I Could" (Red Hot Chili Peppers)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.443_-0.349.dat -nsteps 100000 -x Colvars/traj_-2.443_-0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.905       13.976      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.399        0.019

GROMACS reminds you: "I Feel a Great Disturbance in the Force" (The Emperor Strikes Back)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.443_0.349.dat -nsteps 100000 -x Colvars/traj_-2.443_0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.211       14.303      400.0
                 (ns/day)    (hour/ns)
Performance:     1208.161        0.020

GROMACS reminds you: "Naive you are if you believe life favours those who aren't naive." (Piet Hein)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.443_1.047.dat -nsteps 100000 -x Colvars/traj_-2.443_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.137       14.034      400.0
                 (ns/day)    (hour/ns)
Performance:     1231.291        0.019

GROMACS reminds you: "Jede der Scherben spiegelt das Licht" (Wir sind Helden)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.443_1.745.dat -nsteps 100000 -x Colvars/traj_-2.443_1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.896       13.974      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.597        0.019

GROMACS reminds you: "The Path Of the Righteous Man is Beset On All Sides With the Iniquities Of the Selfish and the Tyranny Of Evil Men." (Pulp Fiction)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.443_2.443.dat -nsteps 100000 -x Colvars/traj_-2.443_2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.718       13.930      400.0
                 (ns/day)    (hour/ns)
Performance:     1240.537        0.019

GROMACS reminds you: "According to my computations we're overdue for a transformation." (Jackson Browne)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.745_-3.142.dat -nsteps 100000 -x Colvars/traj_-1.745_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.774       14.194      400.0
                 (ns/day)    (hour/ns)
Performance:     1217.463        0.020

GROMACS reminds you: "Fifty years of programming language research, and we end up with C++???" (Richard O'Keefe)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.745_-2.443.dat -nsteps 100000 -x Colvars/traj_-1.745_-2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.364       14.091      400.0
                 (ns/day)    (hour/ns)
Performance:     1226.330        0.020

GROMACS reminds you: "GROMACS First : Making MD Great Again" (Vedran Miletic)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.745_-1.745.dat -nsteps 100000 -x Colvars/traj_-1.745_-1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.425       14.106      400.0
                 (ns/day)    (hour/ns)
Performance:     1224.993        0.020

GROMACS reminds you: "Boom Boom Boom Boom, I Want You in My Room" (Venga Boys)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.745_-1.047.dat -nsteps 100000 -x Colvars/traj_-1.745_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.853       14.213      400.0
                 (ns/day)    (hour/ns)
Performance:     1215.783        0.020

GROMACS reminds you: "I Ripped the Cord Right Out Of the Phone" (Capt. Beefheart)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.745_-0.349.dat -nsteps 100000 -x Colvars/traj_-1.745_-0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.668       14.167      400.0
                 (ns/day)    (hour/ns)
Performance:     1219.739        0.020

GROMACS reminds you: "Good Music Saves your Soul" (Lemmy)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.745_0.349.dat -nsteps 100000 -x Colvars/traj_-1.745_0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.008       14.002      400.0
                 (ns/day)    (hour/ns)
Performance:     1234.118        0.019

GROMACS reminds you: "A computer would deserve to be called intelligent if it could deceive a human into believing that it was human." (Alan Turing)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.745_1.047.dat -nsteps 100000 -x Colvars/traj_-1.745_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.124       14.031      400.0
                 (ns/day)    (hour/ns)
Performance:     1231.556        0.019

GROMACS reminds you: "Our two greatest problems are gravity and paper work. We can lick gravity, but sometimes the paperwork is overwhelming." (Wernher von Braun)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.745_1.745.dat -nsteps 100000 -x Colvars/traj_-1.745_1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.614       13.904      400.0
                 (ns/day)    (hour/ns)
Performance:     1242.861        0.019

GROMACS reminds you: "All Beauty Must Die" (Nick Cave)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.745_2.443.dat -nsteps 100000 -x Colvars/traj_-1.745_2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.734       13.934      400.0
                 (ns/day)    (hour/ns)
Performance:     1240.174        0.019

GROMACS reminds you: "I went to Venice and looked at the paintings of Canaletto to understand how he presented perspective, and it turned out it was an exponential law. If I had published this, maybe there would be a Karplus law in art theory as well as the Karplus equation in NMR" (Martin Karplus, Nobel lecture 2013)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_-3.142.dat -nsteps 100000 -x Colvars/traj_-1.047_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-1.047_-3.142.xtc to Colvars/#traj_-1.047_-3.142.xtc.2#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.898       14.224      400.0
                 (ns/day)    (hour/ns)
Performance:     1214.823        0.020

GROMACS reminds you: "Prior to 1965 there were none, and after 1965 there was a nun." (Sister Mary Kenneth Keller regarding women with PhDs in computer science)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_-2.443.dat -nsteps 100000 -x Colvars/traj_-1.047_-2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.204       14.051      400.0
                 (ns/day)    (hour/ns)
Performance:     1229.811        0.020

GROMACS reminds you: "You Don't Wanna Know" (Pulp Fiction)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_-1.745.dat -nsteps 100000 -x Colvars/traj_-1.047_-1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.093       14.023      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.235        0.019

GROMACS reminds you: "Developer accused of unreadable code refuses to comment" (Molly Struve)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_-1.047.dat -nsteps 100000 -x Colvars/traj_-1.047_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-1.047_-1.047.xtc to Colvars/#traj_-1.047_-1.047.xtc.2#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.610       14.153      400.0
                 (ns/day)    (hour/ns)
Performance:     1220.993        0.020

GROMACS reminds you: "Uh-oh" (Tinky Winky)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_-0.349.dat -nsteps 100000 -x Colvars/traj_-1.047_-0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.091       14.023      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.295        0.019

GROMACS reminds you: "Safety lights are for dudes" (Ghostbusters 2016)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_0.349.dat -nsteps 100000 -x Colvars/traj_-1.047_0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.918       14.230      400.0
                 (ns/day)    (hour/ns)
Performance:     1214.386        0.020

GROMACS reminds you: "If everything seems under control, you're just not going fast enough." (Mario Andretti)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_1.047.dat -nsteps 100000 -x Colvars/traj_-1.047_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-1.047_1.047.xtc to Colvars/#traj_-1.047_1.047.xtc.2#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.517       13.879      400.0
                 (ns/day)    (hour/ns)
Performance:     1245.023        0.019

GROMACS reminds you: "Protons give an atom its identity, electrons its personality." (Bill Bryson)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_1.745.dat -nsteps 100000 -x Colvars/traj_-1.047_1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.615       13.904      400.0
                 (ns/day)    (hour/ns)
Performance:     1242.846        0.019

GROMACS reminds you: "Check Your Output" (P. Ahlstrom)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.047_2.443.dat -nsteps 100000 -x Colvars/traj_-1.047_2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.161       14.040      400.0
                 (ns/day)    (hour/ns)
Performance:     1230.762        0.020

GROMACS reminds you: "He's using code that only you and I know" (Kate Bush)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.349_-3.142.dat -nsteps 100000 -x Colvars/traj_-0.349_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.621       14.155      400.0
                 (ns/day)    (hour/ns)
Performance:     1220.748        0.020

GROMACS reminds you: "Art For Arts Sake, Money For Gods Sake" (10 CC)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.349_-2.443.dat -nsteps 100000 -x Colvars/traj_-0.349_-2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       54.552       13.638      400.0
                 (ns/day)    (hour/ns)
Performance:     1267.057        0.019

GROMACS reminds you: "With four parameters I can fit an elephant, and with five I can make him wiggle his trunk." (John von Neumann)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.349_-1.745.dat -nsteps 100000 -x Colvars/traj_-0.349_-1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.773       13.943      400.0
                 (ns/day)    (hour/ns)
Performance:     1239.321        0.019

GROMACS reminds you: "Does All This Money Really Have To Go To Charity ?" (Rick)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.349_-1.047.dat -nsteps 100000 -x Colvars/traj_-0.349_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.842       13.961      400.0
                 (ns/day)    (hour/ns)
Performance:     1237.783        0.019

GROMACS reminds you: "Furious activity is no substitute for understanding." (H.H. Williams)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.349_-0.349.dat -nsteps 100000 -x Colvars/traj_-0.349_-0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.263       14.066      400.0
                 (ns/day)    (hour/ns)
Performance:     1228.520        0.020

GROMACS reminds you: "The three principal virtues of a programmer are Laziness, Impatience, and Hubris" (Larry Wall)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.349_0.349.dat -nsteps 100000 -x Colvars/traj_-0.349_0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.926       14.232      400.0
                 (ns/day)    (hour/ns)
Performance:     1214.207        0.020

GROMACS reminds you: "Never, I said never, compare with experiment" (Magnus Bergh)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.349_1.047.dat -nsteps 100000 -x Colvars/traj_-0.349_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.332       14.083      400.0
                 (ns/day)    (hour/ns)
Performance:     1227.008        0.020

GROMACS reminds you: "Forcefields are like dating; things go fine for a while and then sometimes it goes really bad." (Alex MacKerell)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.349_1.745.dat -nsteps 100000 -x Colvars/traj_-0.349_1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.284       14.321      400.0
                 (ns/day)    (hour/ns)
Performance:     1206.637        0.020

GROMACS reminds you: "You're Insignificant" (Tricky)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.349_2.443.dat -nsteps 100000 -x Colvars/traj_-0.349_2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.300       14.075      400.0
                 (ns/day)    (hour/ns)
Performance:     1227.707        0.020

GROMACS reminds you: "Whatever Happened to Pong ?" (F. Black)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.349_-3.142.dat -nsteps 100000 -x Colvars/traj_0.349_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.677       14.169      400.0
                 (ns/day)    (hour/ns)
Performance:     1219.548        0.020

GROMACS reminds you: "Even if you are on the right track, you will get run over if you just sit there." (Will Rogers)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.349_-2.443.dat -nsteps 100000 -x Colvars/traj_0.349_-2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.958       13.990      400.0
                 (ns/day)    (hour/ns)
Performance:     1235.218        0.019

GROMACS reminds you: "If at one time or another I have brushed a few colleagues the wrong way, I must apologize: I had not realized that they were covered with fur." (Edwin Chargaff)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.349_-1.745.dat -nsteps 100000 -x Colvars/traj_0.349_-1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.550       14.138      400.0
                 (ns/day)    (hour/ns)
Performance:     1222.284        0.020

GROMACS reminds you: "Push It Real Good" (Salt 'n' Pepa)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.349_-1.047.dat -nsteps 100000 -x Colvars/traj_0.349_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.013       14.003      400.0
                 (ns/day)    (hour/ns)
Performance:     1234.015        0.019

GROMACS reminds you: "In ancient times they had no statistics so they had to fall back on lies." (Stephen Leacock)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.349_-0.349.dat -nsteps 100000 -x Colvars/traj_0.349_-0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.532       13.883      400.0
                 (ns/day)    (hour/ns)
Performance:     1244.693        0.019

GROMACS reminds you: "If every study was groundbreaking, we'd end up with a bunch of holes in the ground and nothing built." (Anonymous)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.349_0.349.dat -nsteps 100000 -x Colvars/traj_0.349_0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.691       14.173      400.0
                 (ns/day)    (hour/ns)
Performance:     1219.243        0.020

GROMACS reminds you: "Baby, It Aint Over Till It's Over" (Lenny Kravitz)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.349_1.047.dat -nsteps 100000 -x Colvars/traj_0.349_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       53.576       13.394      400.0
                 (ns/day)    (hour/ns)
Performance:     1290.136        0.019

GROMACS reminds you: "I love fools' experiments. I am always making them." (Charles Darwin)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.349_1.745.dat -nsteps 100000 -x Colvars/traj_0.349_1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.572       13.893      400.0
                 (ns/day)    (hour/ns)
Performance:     1243.809        0.019

GROMACS reminds you: "There's no way you can rely on an experiment" (Gerrit Groenhof)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.349_2.443.dat -nsteps 100000 -x Colvars/traj_0.349_2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.693       13.923      400.0
                 (ns/day)    (hour/ns)
Performance:     1241.107        0.019

GROMACS reminds you: "Why Do *You* Use Constraints ?" (H.J.C. Berendsen)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_-3.142.dat -nsteps 100000 -x Colvars/traj_1.047_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_1.047_-3.142.xtc to Colvars/#traj_1.047_-3.142.xtc.2#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.816       14.204      400.0
                 (ns/day)    (hour/ns)
Performance:     1216.562        0.020

GROMACS reminds you: "Research ! A mere excuse for idleness; it has never achieved, and will never achieve any results of the slightest value." (Benjamin Jowett, British theologian, 1817-93)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_-2.443.dat -nsteps 100000 -x Colvars/traj_1.047_-2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.354       14.089      400.0
                 (ns/day)    (hour/ns)
Performance:     1226.542        0.020

GROMACS reminds you: "If we knew what it was we were doing, it would not be called research, would it?" (Albert Einstein)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_-1.745.dat -nsteps 100000 -x Colvars/traj_1.047_-1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.991       13.998      400.0
                 (ns/day)    (hour/ns)
Performance:     1234.492        0.019

GROMACS reminds you: "Numbers have life; theyre not just symbols on paper." (Shakuntala Devi)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_-1.047.dat -nsteps 100000 -x Colvars/traj_1.047_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_1.047_-1.047.xtc to Colvars/#traj_1.047_-1.047.xtc.2#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.018       14.004      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.906        0.019

GROMACS reminds you: "Can I have everything louder than everything else?" (Deep Purple)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_-0.349.dat -nsteps 100000 -x Colvars/traj_1.047_-0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.218       14.055      400.0
                 (ns/day)    (hour/ns)
Performance:     1229.501        0.020

GROMACS reminds you: "As Always Your Logic Is Impeccable" (Tuvok)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_0.349.dat -nsteps 100000 -x Colvars/traj_1.047_0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.760       13.940      400.0
                 (ns/day)    (hour/ns)
Performance:     1239.603        0.019

GROMACS reminds you: "You Try to Run the Universe" (Tricky)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_1.047.dat -nsteps 100000 -x Colvars/traj_1.047_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_1.047_1.047.xtc to Colvars/#traj_1.047_1.047.xtc.2#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.516       14.379      400.0
                 (ns/day)    (hour/ns)
Performance:     1201.751        0.020

GROMACS reminds you: "These are Ideas, They are Not Lies" (Magnapop)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_1.745.dat -nsteps 100000 -x Colvars/traj_1.047_1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.923       13.981      400.0
                 (ns/day)    (hour/ns)
Performance:     1235.998        0.019

GROMACS reminds you: "Interfacing Space and Beyond..." (P. J. Harvey)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.047_2.443.dat -nsteps 100000 -x Colvars/traj_1.047_2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.051       14.013      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.180        0.019

GROMACS reminds you: "I'm Not Gonna Die Here !" (Sphere)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.745_-3.142.dat -nsteps 100000 -x Colvars/traj_1.745_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.515       14.129      400.0
                 (ns/day)    (hour/ns)
Performance:     1223.050        0.020

GROMACS reminds you: "I don't know how many of you have ever met Dijkstra, but you probably know that arrogance in computer science is measured in nano-Dijkstras." (Alan Kay)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.745_-2.443.dat -nsteps 100000 -x Colvars/traj_1.745_-2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.199       14.050      400.0
                 (ns/day)    (hour/ns)
Performance:     1229.926        0.020

GROMACS reminds you: "Ich war schwanger, mir gings zum kotzen" (Nina Hagen)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.745_-1.745.dat -nsteps 100000 -x Colvars/traj_1.745_-1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.910       14.228      400.0
                 (ns/day)    (hour/ns)
Performance:     1214.556        0.020

GROMACS reminds you: "I Can't Shake It" (Dinosaur Jr)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.745_-1.047.dat -nsteps 100000 -x Colvars/traj_1.745_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.104       14.026      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.005        0.019

GROMACS reminds you: "Sometimes Life is Obscene" (Black Crowes)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.745_-0.349.dat -nsteps 100000 -x Colvars/traj_1.745_-0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.157       14.039      400.0
                 (ns/day)    (hour/ns)
Performance:     1230.838        0.019

GROMACS reminds you: "It is unfortunate that the authors did not make better use of all the electric power energy that went into these massive computations." (An anonymous referee)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.745_0.349.dat -nsteps 100000 -x Colvars/traj_1.745_0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.927       13.982      400.0
                 (ns/day)    (hour/ns)
Performance:     1235.901        0.019

GROMACS reminds you: "Hang On to Your Ego" (F. Black)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.745_1.047.dat -nsteps 100000 -x Colvars/traj_1.745_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.448       14.112      400.0
                 (ns/day)    (hour/ns)
Performance:     1224.487        0.020

GROMACS reminds you: "Everything is theoretically impossible, until it is done." (Robert Heinlein)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.745_1.745.dat -nsteps 100000 -x Colvars/traj_1.745_1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.785       13.946      400.0
                 (ns/day)    (hour/ns)
Performance:     1239.057        0.019

GROMACS reminds you: "You're like them scientists on TV explaining black holes. More you talk, less I get" (Jess Walter)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.745_2.443.dat -nsteps 100000 -x Colvars/traj_1.745_2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.863       13.966      400.0
                 (ns/day)    (hour/ns)
Performance:     1237.324        0.019

GROMACS reminds you: "Performance and power are great targets for tuning, but really you want to tune for money!" (Erik Lindahl)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.443_-3.142.dat -nsteps 100000 -x Colvars/traj_2.443_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.164       14.041      400.0
                 (ns/day)    (hour/ns)
Performance:     1230.689        0.020

GROMACS reminds you: "I went to Venice and looked at the paintings of Canaletto to understand how he presented perspective, and it turned out it was an exponential law. If I had published this, maybe there would be a Karplus law in art theory as well as the Karplus equation in NMR" (Martin Karplus, Nobel lecture 2013)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.443_-2.443.dat -nsteps 100000 -x Colvars/traj_2.443_-2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.599       14.150      400.0
                 (ns/day)    (hour/ns)
Performance:     1221.237        0.020

GROMACS reminds you: "I cannot think of a single one, not even intelligence." (Enrico Fermi, when asked what characteristics physics Nobel laureates had in common.)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.443_-1.745.dat -nsteps 100000 -x Colvars/traj_2.443_-1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.016       14.254      400.0
                 (ns/day)    (hour/ns)
Performance:     1212.289        0.020

GROMACS reminds you: "A robot will be truly autonomous when you instruct it to go to work and it decides to go to the beach instead." (Brad Templeton)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.443_-1.047.dat -nsteps 100000 -x Colvars/traj_2.443_-1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.551       14.138      400.0
                 (ns/day)    (hour/ns)
Performance:     1222.276        0.020

GROMACS reminds you: "I Am Testing Your Grey Matter" (Red Hot Chili Peppers)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.443_-0.349.dat -nsteps 100000 -x Colvars/traj_2.443_-0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.813       13.953      400.0
                 (ns/day)    (hour/ns)
Performance:     1238.438        0.019

GROMACS reminds you: "Highly organized research is guaranteed to produce nothing new." (Frank Herbert)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.443_0.349.dat -nsteps 100000 -x Colvars/traj_2.443_0.349.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.065       14.016      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.862        0.019

GROMACS reminds you: "Apologies to the astrophysics student I met at a party years ago. When you told me how many hours a day you used 4chan and how much you love it, I gave you a funny look and walked away. Now, a decade later, I realize you were talking about Fortran." (Anonymous)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.443_1.047.dat -nsteps 100000 -x Colvars/traj_2.443_1.047.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.203       14.051      400.0
                 (ns/day)    (hour/ns)
Performance:     1229.838        0.020

GROMACS reminds you: "You Dirty Switch, You're On Again" (The Breeders)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.443_1.745.dat -nsteps 100000 -x Colvars/traj_2.443_1.745.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       53.314       13.329      400.0
                 (ns/day)    (hour/ns)
Performance:     1296.473        0.019

GROMACS reminds you: "It's Because Of the Metric System" (Pulp Fiction)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.443_2.443.dat -nsteps 100000 -x Colvars/traj_2.443_2.443.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.900       13.975      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.492        0.019

GROMACS reminds you: "I Can't Shake It" (Dinosaur Jr)


# Integration error:        1.62

# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-3.142.dat -nsteps 100000 -x Colvars/traj_-3.142_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_-3.142.xtc to Colvars/#traj_-3.142_-3.142.xtc.9#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.160       14.040      400.0
                 (ns/day)    (hour/ns)
Performance:     1230.776        0.019

GROMACS reminds you: "You Crashed Into the Swamps" (Silicon Graphics)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-2.513.dat -nsteps 100000 -x Colvars/traj_-3.142_-2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.620       14.155      400.0
                 (ns/day)    (hour/ns)
Performance:     1220.768        0.020

GROMACS reminds you: "Do You Have a Mind of Your Own ?" (Garbage)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-1.885.dat -nsteps 100000 -x Colvars/traj_-3.142_-1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_-1.885.xtc to Colvars/#traj_-3.142_-1.885.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.816       13.954      400.0
                 (ns/day)    (hour/ns)
Performance:     1238.365        0.019

GROMACS reminds you: "Those people who think they know everything are a great annoyance to those of us who do." (Isaac Asimov)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-1.257.dat -nsteps 100000 -x Colvars/traj_-3.142_-1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.897       14.224      400.0
                 (ns/day)    (hour/ns)
Performance:     1214.841        0.020

GROMACS reminds you: "They Were So Quiet About It" (Pixies)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_-0.628.dat -nsteps 100000 -x Colvars/traj_-3.142_-0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_-0.628.xtc to Colvars/#traj_-3.142_-0.628.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.158       14.040      400.0
                 (ns/day)    (hour/ns)
Performance:     1230.816        0.019

GROMACS reminds you: "Ludwig Boltzmann, who spent much of his life studying statistical mechanics, died in 1906, by his own hand. Paul Ehrenfest, carrying on the same work, died similarly in 1933. Now it is our turn to study statistical mechanics. Perhaps it will be wise to approach the subject cautiously." (David Goodstein)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_0.000.dat -nsteps 100000 -x Colvars/traj_-3.142_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_0.000.xtc to Colvars/#traj_-3.142_0.000.xtc.4#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.150       14.288      400.0
                 (ns/day)    (hour/ns)
Performance:     1209.451        0.020

GROMACS reminds you: "I don't fear death because I don't fear anything I don't understand." (Hedy Lamarr)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_0.628.dat -nsteps 100000 -x Colvars/traj_-3.142_0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_0.628.xtc to Colvars/#traj_-3.142_0.628.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.039       14.010      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.438        0.019

GROMACS reminds you: "A mathematician is a blind man in a dark room looking for a black cat which isn't there." (Charles Darwin)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_1.257.dat -nsteps 100000 -x Colvars/traj_-3.142_1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.167       14.042      400.0
                 (ns/day)    (hour/ns)
Performance:     1230.628        0.020

GROMACS reminds you: "Thou shalt not kill -9" (Anonymous)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_1.885.dat -nsteps 100000 -x Colvars/traj_-3.142_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-3.142_1.885.xtc to Colvars/#traj_-3.142_1.885.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.150       14.037      400.0
                 (ns/day)    (hour/ns)
Performance:     1231.003        0.019

GROMACS reminds you: "I never thought of stopping, and I just hated sleeping. I can't imagine having a better life." (Barbara McClintock)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-3.142_2.513.dat -nsteps 100000 -x Colvars/traj_-3.142_2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.114       14.029      400.0
                 (ns/day)    (hour/ns)
Performance:     1231.778        0.019

GROMACS reminds you: "Never replicate a successful experiment." (Fett's law.)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.513_-3.142.dat -nsteps 100000 -x Colvars/traj_-2.513_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.536       14.134      400.0
                 (ns/day)    (hour/ns)
Performance:     1222.582        0.020

GROMACS reminds you: "She's a Good Sheila Bruce" (Monty Python)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.513_-2.513.dat -nsteps 100000 -x Colvars/traj_-2.513_-2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.030       14.008      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.631        0.019

GROMACS reminds you: "Kissing You is Like Kissing Gravel" (Throwing Muses)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.513_-1.885.dat -nsteps 100000 -x Colvars/traj_-2.513_-1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.164       14.041      400.0
                 (ns/day)    (hour/ns)
Performance:     1230.680        0.020

GROMACS reminds you: "You still have to climb to the shoulders of the giants" (Vedran Miletic)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.513_-1.257.dat -nsteps 100000 -x Colvars/traj_-2.513_-1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.267       14.067      400.0
                 (ns/day)    (hour/ns)
Performance:     1228.427        0.020

GROMACS reminds you: "There's Still Time to Change the Road You're On" (Led Zeppelin)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.513_-0.628.dat -nsteps 100000 -x Colvars/traj_-2.513_-0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.111       14.028      400.0
                 (ns/day)    (hour/ns)
Performance:     1231.855        0.019

GROMACS reminds you: "Throwing the Baby Away With the SPC" (S. Hayward)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.513_0.000.dat -nsteps 100000 -x Colvars/traj_-2.513_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.140       14.035      400.0
                 (ns/day)    (hour/ns)
Performance:     1231.218        0.019

GROMACS reminds you: "Martin [Karplus] had a green laser, Arieh [Warshel] had a red laser, I have a *blue* laser" (Michael Levitt, Nobel lecture 2013)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.513_0.628.dat -nsteps 100000 -x Colvars/traj_-2.513_0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.928       13.982      400.0
                 (ns/day)    (hour/ns)
Performance:     1235.889        0.019

GROMACS reminds you: "Why add prime numbers? Prime numbers are made to be multiplied." (Lev Landau)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.513_1.257.dat -nsteps 100000 -x Colvars/traj_-2.513_1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.365       14.091      400.0
                 (ns/day)    (hour/ns)
Performance:     1226.302        0.020

GROMACS reminds you: "How will I know it's working right?" (MGMT)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.513_1.885.dat -nsteps 100000 -x Colvars/traj_-2.513_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.900       13.975      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.506        0.019

GROMACS reminds you: "It Costs Too Much If It Costs a Lot" (Magnapop)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.513_2.513.dat -nsteps 100000 -x Colvars/traj_-2.513_2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.058       14.264      400.0
                 (ns/day)    (hour/ns)
Performance:     1211.416        0.020

GROMACS reminds you: "It is not critical to add the next quote to a patch release" (Paul Bauer)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.885_-3.142.dat -nsteps 100000 -x Colvars/traj_-1.885_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-1.885_-3.142.xtc to Colvars/#traj_-1.885_-3.142.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.666       14.167      400.0
                 (ns/day)    (hour/ns)
Performance:     1219.791        0.020

GROMACS reminds you: "All You Need is Greed" (Aztec Camera)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.885_-2.513.dat -nsteps 100000 -x Colvars/traj_-1.885_-2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.305       14.076      400.0
                 (ns/day)    (hour/ns)
Performance:     1227.609        0.020

GROMACS reminds you: "The Universe is Somewhere In Here" (J.G.E.M. Fraaije)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.885_-1.885.dat -nsteps 100000 -x Colvars/traj_-1.885_-1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-1.885_-1.885.xtc to Colvars/#traj_-1.885_-1.885.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.100       14.025      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.091        0.019

GROMACS reminds you: "We Don't Bother Anyone" (LIVE)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.885_-1.257.dat -nsteps 100000 -x Colvars/traj_-1.885_-1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.869       13.967      400.0
                 (ns/day)    (hour/ns)
Performance:     1237.190        0.019

GROMACS reminds you: "My Head Goes Pop Pop Pop Pop Pop" (F. Black)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.885_-0.628.dat -nsteps 100000 -x Colvars/traj_-1.885_-0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-1.885_-0.628.xtc to Colvars/#traj_-1.885_-0.628.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.131       14.033      400.0
                 (ns/day)    (hour/ns)
Performance:     1231.421        0.019

GROMACS reminds you: "If mathematical analysis should ever hold a prominent place in chemistry - an aberration which is happily almost impossible - it would occasion a rapid and widespread degeneration of that science." (Aguste Comte, 1830)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.885_0.000.dat -nsteps 100000 -x Colvars/traj_-1.885_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.169       14.042      400.0
                 (ns/day)    (hour/ns)
Performance:     1230.585        0.020

GROMACS reminds you: "If I could remember the names of all these particles, I'd be a botanist." (Albert Einstein)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.885_0.628.dat -nsteps 100000 -x Colvars/traj_-1.885_0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-1.885_0.628.xtc to Colvars/#traj_-1.885_0.628.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.343       14.336      400.0
                 (ns/day)    (hour/ns)
Performance:     1205.378        0.020

GROMACS reminds you: "I'd Be Water If I Could" (Red Hot Chili Peppers)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.885_1.257.dat -nsteps 100000 -x Colvars/traj_-1.885_1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       53.117       13.279      400.0
                 (ns/day)    (hour/ns)
Performance:     1301.277        0.018

GROMACS reminds you: "I Live the Life They Wish They Did" (Tricky)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.885_1.885.dat -nsteps 100000 -x Colvars/traj_-1.885_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-1.885_1.885.xtc to Colvars/#traj_-1.885_1.885.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.647       14.162      400.0
                 (ns/day)    (hour/ns)
Performance:     1220.196        0.020

GROMACS reminds you: "I'm a Wishbone and I'm Breaking" (Pixies)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.885_2.513.dat -nsteps 100000 -x Colvars/traj_-1.885_2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.901       13.975      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.474        0.019

GROMACS reminds you: "Don't You Wish You Never Met Her, Dirty Blue Gene?" (Captain Beefheart)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.257_-3.142.dat -nsteps 100000 -x Colvars/traj_-1.257_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.606       13.902      400.0
                 (ns/day)    (hour/ns)
Performance:     1243.037        0.019

GROMACS reminds you: "Let us not get carried away with our ideas and take our models too seriously" (Nancy Swanson)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.257_-2.513.dat -nsteps 100000 -x Colvars/traj_-1.257_-2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.001       14.250      400.0
                 (ns/day)    (hour/ns)
Performance:     1212.610        0.020

GROMACS reminds you: "People who do QM/MM must be rather patient and enjoy quality over speed" (Kresten Lindorff-Larsen)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.257_-1.885.dat -nsteps 100000 -x Colvars/traj_-1.257_-1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.552       14.138      400.0
                 (ns/day)    (hour/ns)
Performance:     1222.253        0.020

GROMACS reminds you: "Schrdinger's backup: The condition of any backup is unknown until a restore is attempted." (Anonymous)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.257_-1.257.dat -nsteps 100000 -x Colvars/traj_-1.257_-1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.264       14.066      400.0
                 (ns/day)    (hour/ns)
Performance:     1228.500        0.020

GROMACS reminds you: "It's easy to remember: a half a kT is equal to five fourths of a kJ/mol." (Anders Gabrielsson)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.257_-0.628.dat -nsteps 100000 -x Colvars/traj_-1.257_-0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.966       13.992      400.0
                 (ns/day)    (hour/ns)
Performance:     1235.039        0.019

GROMACS reminds you: "Drugs are Bad, mmokay" (South Park)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.257_0.000.dat -nsteps 100000 -x Colvars/traj_-1.257_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.602       14.151      400.0
                 (ns/day)    (hour/ns)
Performance:     1221.158        0.020

GROMACS reminds you: "You Leave Me Dry" (P.J. Harvey)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.257_0.628.dat -nsteps 100000 -x Colvars/traj_-1.257_0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.706       14.176      400.0
                 (ns/day)    (hour/ns)
Performance:     1218.935        0.020

GROMACS reminds you: "If a rat is a good model for your emotional life, you're in big trouble." (Robert Sapolsky)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.257_1.257.dat -nsteps 100000 -x Colvars/traj_-1.257_1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.041       14.010      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.390        0.019

GROMACS reminds you: "It's So Lonely When You Don't Even Know Yourself" (Red Hot Chili Peppers)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.257_1.885.dat -nsteps 100000 -x Colvars/traj_-1.257_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.642       14.160      400.0
                 (ns/day)    (hour/ns)
Performance:     1220.309        0.020

GROMACS reminds you: "Highly organized research is guaranteed to produce nothing new." (Frank Herbert)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.257_2.513.dat -nsteps 100000 -x Colvars/traj_-1.257_2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.750       13.938      400.0
                 (ns/day)    (hour/ns)
Performance:     1239.824        0.019

GROMACS reminds you: "Philosophy of science is about as useful to scientists as ornithology is to birds." (Richard Feynman)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.628_-3.142.dat -nsteps 100000 -x Colvars/traj_-0.628_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-0.628_-3.142.xtc to Colvars/#traj_-0.628_-3.142.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.009       14.002      400.0
                 (ns/day)    (hour/ns)
Performance:     1234.090        0.019

GROMACS reminds you: "Correctomundo" (Pulp Fiction)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.628_-2.513.dat -nsteps 100000 -x Colvars/traj_-0.628_-2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.699       14.175      400.0
                 (ns/day)    (hour/ns)
Performance:     1219.078        0.020

GROMACS reminds you: "I hadn't been aware that there were doors closed to me until I started knocking on them." (Gertrude Elion)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.628_-1.885.dat -nsteps 100000 -x Colvars/traj_-0.628_-1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-0.628_-1.885.xtc to Colvars/#traj_-0.628_-1.885.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.223       14.056      400.0
                 (ns/day)    (hour/ns)
Performance:     1229.388        0.020

GROMACS reminds you: "Misslycka kan man med all kod" (Mats Nylen)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.628_-1.257.dat -nsteps 100000 -x Colvars/traj_-0.628_-1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.925       14.231      400.0
                 (ns/day)    (hour/ns)
Performance:     1214.230        0.020

GROMACS reminds you: "It's easy to remember: a half a kT is equal to five fourths of a kJ/mol." (Anders Gabrielsson)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.628_-0.628.dat -nsteps 100000 -x Colvars/traj_-0.628_-0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-0.628_-0.628.xtc to Colvars/#traj_-0.628_-0.628.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.110       14.028      400.0
                 (ns/day)    (hour/ns)
Performance:     1231.866        0.019

GROMACS reminds you: "Somewhere, something incredible is waiting to be known." (Carl Sagan)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.628_0.000.dat -nsteps 100000 -x Colvars/traj_-0.628_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.850       13.963      400.0
                 (ns/day)    (hour/ns)
Performance:     1237.596        0.019

GROMACS reminds you: "In a talk you have a choice: You can make one point or no points." (Paul Sigler)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.628_0.628.dat -nsteps 100000 -x Colvars/traj_-0.628_0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-0.628_0.628.xtc to Colvars/#traj_-0.628_0.628.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.166       14.042      400.0
                 (ns/day)    (hour/ns)
Performance:     1230.635        0.020

GROMACS reminds you: "Don't Grumble, Give a Whistle !" (Monty Python)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.628_1.257.dat -nsteps 100000 -x Colvars/traj_-0.628_1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       52.467       13.117      400.0
                 (ns/day)    (hour/ns)
Performance:     1317.401        0.018

GROMACS reminds you: "I Was Born to Have Adventure" (F. Zappa)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.628_1.885.dat -nsteps 100000 -x Colvars/traj_-0.628_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_-0.628_1.885.xtc to Colvars/#traj_-0.628_1.885.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.049       14.262      400.0
                 (ns/day)    (hour/ns)
Performance:     1211.601        0.020

GROMACS reminds you: "When I asked a younger colleague at the university how he had been able to change his research field several times within a decade or so, he answered: "It's just a question of new software"" (Paul Verhaeghe)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.628_2.513.dat -nsteps 100000 -x Colvars/traj_-0.628_2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.862       13.966      400.0
                 (ns/day)    (hour/ns)
Performance:     1237.334        0.019

GROMACS reminds you: "Uh-oh" (Tinky Winky)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_-3.142.dat -nsteps 100000 -x Colvars/traj_0.000_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_0.000_-3.142.xtc to Colvars/#traj_0.000_-3.142.xtc.4#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.927       13.982      400.0
                 (ns/day)    (hour/ns)
Performance:     1235.905        0.019

GROMACS reminds you: "In science it often happens that scientists say, 'You know that's a really good argument; my position is mistaken,' and then they would actually change their minds and you never hear that old view from them again. They really do it. It doesn't happen as often as it should, because scientists are human and change is sometimes painful. But it happens every day. I cannot recall the last time something like that happened in politics or religion." (Carl Sagan)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_-2.513.dat -nsteps 100000 -x Colvars/traj_0.000_-2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       53.393       13.348      400.0
                 (ns/day)    (hour/ns)
Performance:     1294.547        0.019

GROMACS reminds you: "We are perhaps not far removed from the time when we shall be able to submit the bulk of chemical phenomena to calculation." (Joseph Gay-Lussac, 1808)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_-1.885.dat -nsteps 100000 -x Colvars/traj_0.000_-1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.731       14.183      400.0
                 (ns/day)    (hour/ns)
Performance:     1218.383        0.020

GROMACS reminds you: "Should we force science down the throats of those that have no taste for it? Is it our duty to drag them kicking and screaming into the twenty-first century? I am afraid that it is." (George Porter)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_-1.257.dat -nsteps 100000 -x Colvars/traj_0.000_-1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.128       14.032      400.0
                 (ns/day)    (hour/ns)
Performance:     1231.467        0.019

GROMACS reminds you: "I cannot think of a single one, not even intelligence." (Enrico Fermi, when asked what characteristics physics Nobel laureates had in common.)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_-0.628.dat -nsteps 100000 -x Colvars/traj_0.000_-0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.795       14.199      400.0
                 (ns/day)    (hour/ns)
Performance:     1217.021        0.020

GROMACS reminds you: "I believe in miracles cause I'm one" (The Ramones)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_0.000.dat -nsteps 100000 -x Colvars/traj_0.000_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_0.000_0.000.xtc to Colvars/#traj_0.000_0.000.xtc.4#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.907       13.977      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.350        0.019

GROMACS reminds you: "Ich war schwanger, mir gings zum kotzen" (Nina Hagen)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_0.628.dat -nsteps 100000 -x Colvars/traj_0.000_0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.746       13.936      400.0
                 (ns/day)    (hour/ns)
Performance:     1239.924        0.019

GROMACS reminds you: "I was taught that the way of progress was neither swift nor easy." (Marie Curie)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_1.257.dat -nsteps 100000 -x Colvars/traj_0.000_1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.794       13.949      400.0
                 (ns/day)    (hour/ns)
Performance:     1238.844        0.019

GROMACS reminds you: "When I got my first job as a programmer there were quite a number of women doing programming because there was no computer science education and people were hired from many different fields if it seemed like they could do the work. It was better then, probably, in terms of proportions, not necessarily in women being paid as much as men and so forth, but definitely, there were women around." (Barbara Liskov)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_1.885.dat -nsteps 100000 -x Colvars/traj_0.000_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.156       14.039      400.0
                 (ns/day)    (hour/ns)
Performance:     1230.873        0.019

GROMACS reminds you: "Do You Have Sex Maniacs or Schizophrenics or Astrophysicists in Your Family?" (Gogol Bordello)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_2.513.dat -nsteps 100000 -x Colvars/traj_0.000_2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       52.455       13.114      400.0
                 (ns/day)    (hour/ns)
Performance:     1317.712        0.018

GROMACS reminds you: "California, R.I.P." (Red Hot Chili Peppars)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.628_-3.142.dat -nsteps 100000 -x Colvars/traj_0.628_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_0.628_-3.142.xtc to Colvars/#traj_0.628_-3.142.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.093       14.023      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.246        0.019

GROMACS reminds you: "What's the point, yo, what's the spread?" (Red Hot Chili Peppers)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.628_-2.513.dat -nsteps 100000 -x Colvars/traj_0.628_-2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.560       14.140      400.0
                 (ns/day)    (hour/ns)
Performance:     1222.071        0.020

GROMACS reminds you: "Years of calculations and the stress, My science is waiting, nearly complete" (Midlake)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.628_-1.885.dat -nsteps 100000 -x Colvars/traj_0.628_-1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_0.628_-1.885.xtc to Colvars/#traj_0.628_-1.885.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.198       14.050      400.0
                 (ns/day)    (hour/ns)
Performance:     1229.947        0.020

GROMACS reminds you: "Don't Push Me, Cause I'm Close to the Edge" (Tricky)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.628_-1.257.dat -nsteps 100000 -x Colvars/traj_0.628_-1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.197       14.299      400.0
                 (ns/day)    (hour/ns)
Performance:     1208.454        0.020

GROMACS reminds you: "You Can Be Too Early, You Can Be Too Late and You Can Be On Time" (J. Cruijff)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.628_-0.628.dat -nsteps 100000 -x Colvars/traj_0.628_-0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_0.628_-0.628.xtc to Colvars/#traj_0.628_-0.628.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.150       14.038      400.0
                 (ns/day)    (hour/ns)
Performance:     1230.988        0.019

GROMACS reminds you: "I don't know how many of you have ever met Dijkstra, but you probably know that arrogance in computer science is measured in nano-Dijkstras." (Alan Kay)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.628_0.000.dat -nsteps 100000 -x Colvars/traj_0.628_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.914       14.229      400.0
                 (ns/day)    (hour/ns)
Performance:     1214.467        0.020

GROMACS reminds you: "God is a DJ" (Faithless)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.628_0.628.dat -nsteps 100000 -x Colvars/traj_0.628_0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_0.628_0.628.xtc to Colvars/#traj_0.628_0.628.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.176       14.294      400.0
                 (ns/day)    (hour/ns)
Performance:     1208.895        0.020

GROMACS reminds you: "When I got my first job as a programmer there were quite a number of women doing programming because there was no computer science education and people were hired from many different fields if it seemed like they could do the work. It was better then, probably, in terms of proportions, not necessarily in women being paid as much as men and so forth, but definitely, there were women around." (Barbara Liskov)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.628_1.257.dat -nsteps 100000 -x Colvars/traj_0.628_1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       52.766       13.192      400.0
                 (ns/day)    (hour/ns)
Performance:     1309.931        0.018

GROMACS reminds you: "Right Between the Eyes" (F. Zappa)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.628_1.885.dat -nsteps 100000 -x Colvars/traj_0.628_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_0.628_1.885.xtc to Colvars/#traj_0.628_1.885.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.531       14.133      400.0
                 (ns/day)    (hour/ns)
Performance:     1222.702        0.020

GROMACS reminds you: "Fifty years of programming language research, and we end up with C++???" (Richard O'Keefe)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.628_2.513.dat -nsteps 100000 -x Colvars/traj_0.628_2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.407       14.102      400.0
                 (ns/day)    (hour/ns)
Performance:     1225.396        0.020

GROMACS reminds you: "I ought to warn you, I have no faith" (Jane Eyre in Jane Eyre by Charlotte Bronte)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.257_-3.142.dat -nsteps 100000 -x Colvars/traj_1.257_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.086       14.022      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.389        0.019

GROMACS reminds you: "You Could Be a Shadow" (The Breeders)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.257_-2.513.dat -nsteps 100000 -x Colvars/traj_1.257_-2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.720       14.180      400.0
                 (ns/day)    (hour/ns)
Performance:     1218.620        0.020

GROMACS reminds you: "Don't Push Me, Cause I'm Close to the Edge" (Grandmaster Flash)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.257_-1.885.dat -nsteps 100000 -x Colvars/traj_1.257_-1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.957       13.989      400.0
                 (ns/day)    (hour/ns)
Performance:     1235.252        0.019

GROMACS reminds you: "We can make it into a friend class. But I don't like having friends." (Joe Jordan)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.257_-1.257.dat -nsteps 100000 -x Colvars/traj_1.257_-1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       52.904       13.226      400.0
                 (ns/day)    (hour/ns)
Performance:     1306.516        0.018

GROMACS reminds you: "In ancient times they had no statistics so they had to fall back on lies." (Stephen Leacock)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.257_-0.628.dat -nsteps 100000 -x Colvars/traj_1.257_-0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.779       13.945      400.0
                 (ns/day)    (hour/ns)
Performance:     1239.172        0.019

GROMACS reminds you: "You should call it 'entropy'. No one knows what entropy really is, so in a debate you will always have the advantage." (John von Neumann to Claude Shannon, on why he should borrow the term for information theory)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.257_0.000.dat -nsteps 100000 -x Colvars/traj_1.257_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.882       13.971      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.892        0.019

GROMACS reminds you: "Safety lights are for dudes" (Ghostbusters 2016)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.257_0.628.dat -nsteps 100000 -x Colvars/traj_1.257_0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.363       13.841      400.0
                 (ns/day)    (hour/ns)
Performance:     1248.485        0.019

GROMACS reminds you: "A ship in port is safe, but that is not what ships are for. Sail out to sea and do new things." (Grace Hopper, developer of COBOL)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.257_1.257.dat -nsteps 100000 -x Colvars/traj_1.257_1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.607       14.152      400.0
                 (ns/day)    (hour/ns)
Performance:     1221.049        0.020

GROMACS reminds you: "The easiest way to scale well is to have bad single-core performance" (Blind Freddie)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.257_1.885.dat -nsteps 100000 -x Colvars/traj_1.257_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.619       14.155      400.0
                 (ns/day)    (hour/ns)
Performance:     1220.789        0.020

GROMACS reminds you: "It has been discovered that C++ provides a remarkable facility for concealing the trivial details of a program - such as where its bugs are." (David Keppel)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.257_2.513.dat -nsteps 100000 -x Colvars/traj_1.257_2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.357       14.089      400.0
                 (ns/day)    (hour/ns)
Performance:     1226.480        0.020

GROMACS reminds you: "You're Insignificant" (Tricky)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.885_-3.142.dat -nsteps 100000 -x Colvars/traj_1.885_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_1.885_-3.142.xtc to Colvars/#traj_1.885_-3.142.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.334       14.084      400.0
                 (ns/day)    (hour/ns)
Performance:     1226.979        0.020

GROMACS reminds you: "I'm no model lady. A model's just an imitation of the real thing." (Mae West)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.885_-2.513.dat -nsteps 100000 -x Colvars/traj_1.885_-2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.441       14.110      400.0
                 (ns/day)    (hour/ns)
Performance:     1224.657        0.020

GROMACS reminds you: "Forcefields are like dating; things go fine for a while and then sometimes it goes really bad." (Alex MacKerell)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.885_-1.885.dat -nsteps 100000 -x Colvars/traj_1.885_-1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_1.885_-1.885.xtc to Colvars/#traj_1.885_-1.885.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.922       13.981      400.0
                 (ns/day)    (hour/ns)
Performance:     1236.009        0.019

GROMACS reminds you: "I have had my results for a long time, but I do not yet know how I am to arrive at them." (Carl Friedrich Gauss)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.885_-1.257.dat -nsteps 100000 -x Colvars/traj_1.885_-1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.022       14.006      400.0
                 (ns/day)    (hour/ns)
Performance:     1233.799        0.019

GROMACS reminds you: "(That makes 100 errors; please try again.)" (TeX)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.885_-0.628.dat -nsteps 100000 -x Colvars/traj_1.885_-0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_1.885_-0.628.xtc to Colvars/#traj_1.885_-0.628.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       57.619       14.405      400.0
                 (ns/day)    (hour/ns)
Performance:     1199.602        0.020

GROMACS reminds you: "Meet Me At the Coffee Shop" (Red Hot Chili Peppers)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.885_0.000.dat -nsteps 100000 -x Colvars/traj_1.885_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.600       14.150      400.0
                 (ns/day)    (hour/ns)
Performance:     1221.209        0.020

GROMACS reminds you: "Oh My God ! It's the Funky Shit" (Beastie Boys)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.885_0.628.dat -nsteps 100000 -x Colvars/traj_1.885_0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_1.885_0.628.xtc to Colvars/#traj_1.885_0.628.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.476       14.119      400.0
                 (ns/day)    (hour/ns)
Performance:     1223.881        0.020

GROMACS reminds you: "You Could Make More Money As a Butcher" (F. Zappa)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.885_1.257.dat -nsteps 100000 -x Colvars/traj_1.885_1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.733       13.933      400.0
                 (ns/day)    (hour/ns)
Performance:     1240.207        0.019

GROMACS reminds you: "Still I had a lurking question. Would it not be better if one could really 'see' whether molecules as complicated as the sterols, or strychnine were just as experiment suggested?" (Dorothy Hodgkin)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.885_1.885.dat -nsteps 100000 -x Colvars/traj_1.885_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up Colvars/traj_1.885_1.885.xtc to Colvars/#traj_1.885_1.885.xtc.1#

Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.123       14.031      400.0
                 (ns/day)    (hour/ns)
Performance:     1231.594        0.019

GROMACS reminds you: "BioBeat is Not Available In Regular Shops" (P.J. Meulenhoff)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.885_2.513.dat -nsteps 100000 -x Colvars/traj_1.885_2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.746       14.186      400.0
                 (ns/day)    (hour/ns)
Performance:     1218.074        0.020

GROMACS reminds you: "Everybody is Smashing Things Down" (Offspring)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.513_-3.142.dat -nsteps 100000 -x Colvars/traj_2.513_-3.142.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.822       13.955      400.0
                 (ns/day)    (hour/ns)
Performance:     1238.238        0.019

GROMACS reminds you: "Player Sleeps With the Fishes" (Ein Bekanntes Spiel Von ID Software)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.513_-2.513.dat -nsteps 100000 -x Colvars/traj_2.513_-2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       55.768       13.942      400.0
                 (ns/day)    (hour/ns)
Performance:     1239.438        0.019

GROMACS reminds you: "Please implement proper hep writing" (GROMACS)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.513_-1.885.dat -nsteps 100000 -x Colvars/traj_2.513_-1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       53.269       13.317      400.0
                 (ns/day)    (hour/ns)
Performance:     1297.575        0.018

GROMACS reminds you: "Catholic School Girls Rule" (Red Hot Chili Peppers)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.513_-1.257.dat -nsteps 100000 -x Colvars/traj_2.513_-1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.192       14.048      400.0
                 (ns/day)    (hour/ns)
Performance:     1230.068        0.020

GROMACS reminds you: "The road to openness is paved with git commits" (Vedran Miletic)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.513_-0.628.dat -nsteps 100000 -x Colvars/traj_2.513_-0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       53.022       13.256      400.0
                 (ns/day)    (hour/ns)
Performance:     1303.622        0.018

GROMACS reminds you: "You Can Always Go On Ricky Lake" (Offspring)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.513_0.000.dat -nsteps 100000 -x Colvars/traj_2.513_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.701       14.175      400.0
                 (ns/day)    (hour/ns)
Performance:     1219.022        0.020

GROMACS reminds you: "If it weren't for C, we'd all be programming in BASI and OBOL." (Anonymous)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.513_0.628.dat -nsteps 100000 -x Colvars/traj_2.513_0.628.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.447       14.112      400.0
                 (ns/day)    (hour/ns)
Performance:     1224.522        0.020

GROMACS reminds you: "If You Don't Like Cool Quotes Check Your GMXRC File" (Your Sysadmin)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.513_1.257.dat -nsteps 100000 -x Colvars/traj_2.513_1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.065       14.016      400.0
                 (ns/day)    (hour/ns)
Performance:     1232.851        0.019

GROMACS reminds you: "Please implement proper hep writing" (GROMACS)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.513_1.885.dat -nsteps 100000 -x Colvars/traj_2.513_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 12 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.929       14.232      400.0
                 (ns/day)    (hour/ns)
Performance:     1214.152        0.020

GROMACS reminds you: "Life in the streets is not easy" (Marky Mark)

                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131126
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.513_2.513.dat -nsteps 100000 -x Colvars/traj_2.513_2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 4 OpenMP threads 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

NOTE: 13 % of the run time was spent in domain decomposition,
      3 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:       56.404       14.101      400.0
                 (ns/day)    (hour/ns)
Performance:     1225.447        0.020

GROMACS reminds you: "Still I had a lurking question. Would it not be better if one could really 'see' whether molecules as complicated as the sterols, or strychnine were just as experiment suggested?" (Dorothy Hodgkin)


# Integration error:        1.99

# Integrating             - Simpson's rule + Real Space Grid Mini 
# Integration error:        1.33

