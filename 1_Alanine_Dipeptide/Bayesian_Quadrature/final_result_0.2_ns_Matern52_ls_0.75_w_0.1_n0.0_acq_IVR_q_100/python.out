rm: cannot remove '*#*': No such file or directory
                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.508_1.194.dat -nsteps 100000 -x Colvars/traj_-1.508_1.194.xtc

Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 

starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

               Core t (s)   Wall t (s)        (%)
       Time:       15.036       15.036      100.0
                 (ns/day)    (hour/ns)
Performance:     1149.250        0.021

GROMACS reminds you: "Art For Arts Sake, Money For Gods Sake" (10 CC)

rm: cannot remove '*#*': No such file or directory
                 :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.880_-0.880.dat -nsteps 100000 -x Colvars/traj_0.880_-0.880.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.870       14.870      100.0
                 (ns/day)    (hour/ns)
Performance:     1162.117        0.021

GROMACS reminds you: "There's only music to make new ringtones" (Arctic Monkeys)

 /home/ekempke/miniconda3/envs/emukit_env2/lib/python3.11/site-packages/paramz/transformations.py:111: RuntimeWarning:divide by zero encountered in log
Error occurred while running command: rm *#*
Command 'rm *#*' returned non-zero exit status 1.
Error occurred while running command: rm *#*
Command 'rm *#*' returned non-zero exit status 1.
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.257_1.445.dat -nsteps 100000 -x Colvars/traj_1.257_1.445.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.983       14.983      100.0
                 (ns/day)    (hour/ns)
Performance:     1153.299        0.021

GROMACS reminds you: "Ich war schwanger, mir gings zum kotzen" (Nina Hagen)


# Integration error:        0.44

 -------------- BaysOpt loop, query 1 ---------------
going to run a simulation at 1.2566370609999933 1.4451326210000188 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.382_-1.319.dat -nsteps 100000 -x Colvars/traj_-1.382_-1.319.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.725       14.725      100.0
                 (ns/day)    (hour/ns)
Performance:     1173.528        0.020

GROMACS reminds you: "Sacrifices must be made" (Otto Lilienthal, dying after having crashed with his glider in 1896)


# Integration error:        1.56

 -------------- BaysOpt loop, query 2 ---------------
going to run a simulation at -1.3823007680000294 -1.3194689150000116 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.011_-2.011.dat -nsteps 100000 -x Colvars/traj_2.011_-2.011.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.750       14.750      100.0
                 (ns/day)    (hour/ns)
Performance:     1171.519        0.020

GROMACS reminds you: "Everybody is Smashing Things Down" (Offspring)


# Integration error:        1.85

 -------------- BaysOpt loop, query 3 ---------------
going to run a simulation at 2.0106192979999813 -2.0106192979999813 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.251_2.136.dat -nsteps 100000 -x Colvars/traj_-0.251_2.136.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.901       14.901      100.0
                 (ns/day)    (hour/ns)
Performance:     1159.653        0.021

GROMACS reminds you: "Science... never solves a problem without creating ten more." (George Bernard Shaw)


# Integration error:        2.79

 -------------- BaysOpt loop, query 4 ---------------
going to run a simulation at -0.25132741199999636 2.1362830039999965 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.262_0.126.dat -nsteps 100000 -x Colvars/traj_2.262_0.126.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.794       14.794      100.0
                 (ns/day)    (hour/ns)
Performance:     1168.028        0.021

GROMACS reminds you: "Apologies to the astrophysics student I met at a party years ago. When you told me how many hours a day you used 4chan and how much you love it, I gave you a funny look and walked away. Now, a decade later, I realize you were talking about Fortran." (Anonymous)


# Integration error:        3.64

 -------------- BaysOpt loop, query 5 ---------------
going to run a simulation at 2.2619467110000038 0.12566370599999818 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.314_0.126.dat -nsteps 100000 -x Colvars/traj_-0.314_0.126.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.925       14.925      100.0
                 (ns/day)    (hour/ns)
Performance:     1157.790        0.021

GROMACS reminds you: "I always seem to get inspiration and renewed vitality by contact with this great novel land of yours which sticks up out of the Atlantic." (Winston Churchill)


# Integration error:        2.62

 -------------- BaysOpt loop, query 6 ---------------
going to run a simulation at -0.31415926500000063 0.12566370599999818 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_-2.262.dat -nsteps 100000 -x Colvars/traj_0.000_-2.262.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.837       14.837      100.0
                 (ns/day)    (hour/ns)
Performance:     1164.698        0.021

GROMACS reminds you: "The use of COBOL cripples the mind; its teaching should therefore be regarded as a criminal offense." (Edsger Dijkstra)


# Integration error:        2.38

 -------------- BaysOpt loop, query 7 ---------------
going to run a simulation at 0.0 -2.2619467110000038 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.388_-0.126.dat -nsteps 100000 -x Colvars/traj_-2.388_-0.126.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.857       14.857      100.0
                 (ns/day)    (hour/ns)
Performance:     1163.111        0.021

GROMACS reminds you: "I Do It All the Time" (Magnapop)


# Integration error:        5.11

 -------------- BaysOpt loop, query 8 ---------------
going to run a simulation at -2.3876104169999515 -0.12566370599999818 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.262_2.325.dat -nsteps 100000 -x Colvars/traj_-2.262_2.325.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.671       14.671      100.0
                 (ns/day)    (hour/ns)
Performance:     1177.860        0.020

GROMACS reminds you: "Player Sleeps With the Fishes" (Ein Bekanntes Spiel Von ID Software)


# Integration error:        4.57

 -------------- BaysOpt loop, query 9 ---------------
going to run a simulation at -2.2619467110000038 2.3247785640000505 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.262_-2.325.dat -nsteps 100000 -x Colvars/traj_-2.262_-2.325.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.801       14.801      100.0
                 (ns/day)    (hour/ns)
Performance:     1167.535        0.021

GROMACS reminds you: "Furious activity is no substitute for understanding." (H.H. Williams)


# Integration error:        4.62

 -------------- BaysOpt loop, query 10 ---------------
going to run a simulation at -2.2619467110000038 -2.3247785640000505 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.325_2.262.dat -nsteps 100000 -x Colvars/traj_2.325_2.262.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.372       15.372      100.0
                 (ns/day)    (hour/ns)
Performance:     1124.160        0.021

GROMACS reminds you: "Rockets are cool. There's no getting around that." (Elon Musk)


# Integration error:        5.23

 -------------- BaysOpt loop, query 11 ---------------
going to run a simulation at 2.3247785640000505 2.2619467110000038 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.942_2.639.dat -nsteps 100000 -x Colvars/traj_0.942_2.639.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.668       14.668      100.0
                 (ns/day)    (hour/ns)
Performance:     1178.101        0.020

GROMACS reminds you: "Scientists think they are born with logic; God forbid they should study this discipline with a history of more than two and a half millennia." (Roald Hoffmann)


# Integration error:        5.41

 -------------- BaysOpt loop, query 12 ---------------
going to run a simulation at 0.9424777960000189 2.6389378290000334 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.817_0.503.dat -nsteps 100000 -x Colvars/traj_0.817_0.503.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.030       15.030      100.0
                 (ns/day)    (hour/ns)
Performance:     1149.745        0.021

GROMACS reminds you: "Prior to 1965 there were none, and after 1965 there was a nun." (Sister Mary Kenneth Keller regarding women with PhDs in computer science)


# Integration error:        6.47

 -------------- BaysOpt loop, query 13 ---------------
going to run a simulation at 0.8168140899999872 0.5026548249999909 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.576_-1.005.dat -nsteps 100000 -x Colvars/traj_2.576_-1.005.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.846       14.846      100.0
                 (ns/day)    (hour/ns)
Performance:     1163.959        0.021

GROMACS reminds you: "We all understand the twinge of discomfort at the thought that we share a common ancestor with the apes. No one can embarrass you like a relative." (Neal DeGrasse Tyson)


# Integration error:        5.57

 -------------- BaysOpt loop, query 14 ---------------
going to run a simulation at 2.576105975999961 -1.0053096489999906 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.194_-2.576.dat -nsteps 100000 -x Colvars/traj_1.194_-2.576.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.751       14.751      100.0
                 (ns/day)    (hour/ns)
Performance:     1171.431        0.020

GROMACS reminds you: "It is disconcerting to reflect on the number of students we have flunked in chemistry for not knowing what we later found to be untrue." (Robert L. Weber)


# Integration error:        7.60

 -------------- BaysOpt loop, query 15 ---------------
going to run a simulation at 1.1938052079999777 -2.576105975999961 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.314_-1.068.dat -nsteps 100000 -x Colvars/traj_-0.314_-1.068.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.925       14.925      100.0
                 (ns/day)    (hour/ns)
Performance:     1157.804        0.021

GROMACS reminds you: "It was something to at least have a choice of nightmares" (Joseph Conrad)


# Integration error:        8.15

 -------------- BaysOpt loop, query 16 ---------------
going to run a simulation at -0.31415926500000063 -1.0681415019999982 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.068_-2.639.dat -nsteps 100000 -x Colvars/traj_-1.068_-2.639.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.551       14.551      100.0
                 (ns/day)    (hour/ns)
Performance:     1187.589        0.020

GROMACS reminds you: "It doesn't pay to make predictions" (Crowded House)


# Integration error:        8.62

 -------------- BaysOpt loop, query 17 ---------------
going to run a simulation at -1.0681415019999982 -2.6389378290000334 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.576_1.131.dat -nsteps 100000 -x Colvars/traj_2.576_1.131.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.851       14.851      100.0
                 (ns/day)    (hour/ns)
Performance:     1163.550        0.021

GROMACS reminds you: "The programmer got stuck in the shower because the instructions on the shampoo bottle said: Lather, Rinse, Repeat." (Anonymous)


# Integration error:        7.18

 -------------- BaysOpt loop, query 18 ---------------
going to run a simulation at 2.576105975999961 1.130973355000006 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.702_1.131.dat -nsteps 100000 -x Colvars/traj_-2.702_1.131.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.080       15.080      100.0
                 (ns/day)    (hour/ns)
Performance:     1145.894        0.021

GROMACS reminds you: "Mathematics is no more computation than typing is literature." (John Allen Paulos)


# Integration error:        6.37

 -------------- BaysOpt loop, query 19 ---------------
going to run a simulation at -2.701769681999987 1.130973355000006 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.576_-1.257.dat -nsteps 100000 -x Colvars/traj_-2.576_-1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.713       14.713      100.0
                 (ns/day)    (hour/ns)
Performance:     1174.455        0.020

GROMACS reminds you: "The scientist is not the person who always gives the right answers, he is the one who asks the right questions." (Claude Levi-Strauss)


# Integration error:        6.52

 -------------- BaysOpt loop, query 20 ---------------
going to run a simulation at -2.576105975999961 -1.2566370609999933 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.194_2.639.dat -nsteps 100000 -x Colvars/traj_-1.194_2.639.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.824       14.824      100.0
                 (ns/day)    (hour/ns)
Performance:     1165.708        0.021

GROMACS reminds you: "Don't waste pure thoughts on dirty enzymes." (Efraim Racker)


# Integration error:        6.57

 -------------- BaysOpt loop, query 21 ---------------
going to run a simulation at -1.1938052079999777 2.6389378290000334 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.319_0.000.dat -nsteps 100000 -x Colvars/traj_-1.319_0.000.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.615       14.615      100.0
                 (ns/day)    (hour/ns)
Performance:     1182.335        0.020

GROMACS reminds you: "The secret to getting ahead is getting started" (Mark Twain)


# Integration error:        6.55

 -------------- BaysOpt loop, query 22 ---------------
going to run a simulation at -1.3194689150000116 0.0 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.251_1.194.dat -nsteps 100000 -x Colvars/traj_-0.251_1.194.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.917       14.917      100.0
                 (ns/day)    (hour/ns)
Performance:     1158.392        0.021

GROMACS reminds you: "In a talk you have a choice: You can make one point or no points." (Paul Sigler)


# Integration error:        6.42

 -------------- BaysOpt loop, query 23 ---------------
going to run a simulation at -0.25132741199999636 1.1938052079999777 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.702_-2.576.dat -nsteps 100000 -x Colvars/traj_2.702_-2.576.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.651       14.651      100.0
                 (ns/day)    (hour/ns)
Performance:     1179.468        0.020

GROMACS reminds you: "This is Tense !" (Star Wars Episode I The Phantom Menace)


# Integration error:        6.40

 -------------- BaysOpt loop, query 24 ---------------
going to run a simulation at 2.701769681999987 -2.576105975999961 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.634_-0.817.dat -nsteps 100000 -x Colvars/traj_1.634_-0.817.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.880       14.880      100.0
                 (ns/day)    (hour/ns)
Performance:     1161.284        0.021

GROMACS reminds you: "A C program is like a fast dance on a newly waxed dance floor by people carrying razors." (Waldi Ravens)


# Integration error:        5.45

 -------------- BaysOpt loop, query 25 ---------------
going to run a simulation at 1.6336281799999743 -0.8168140899999872 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.817_-1.696.dat -nsteps 100000 -x Colvars/traj_0.817_-1.696.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.196       15.196      100.0
                 (ns/day)    (hour/ns)
Performance:     1137.120        0.021

GROMACS reminds you: "Way to Go Dude" (Beavis and Butthead)


# Integration error:        4.58

 -------------- BaysOpt loop, query 26 ---------------
going to run a simulation at 0.8168140899999872 -1.6964600329999944 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.639_2.890.dat -nsteps 100000 -x Colvars/traj_2.639_2.890.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.449       14.449      100.0
                 (ns/day)    (hour/ns)
Performance:     1195.949        0.020

GROMACS reminds you: "Sometimes Life is Obscene" (Black Crowes)


# Integration error:        4.60

 -------------- BaysOpt loop, query 27 ---------------
going to run a simulation at 2.6389378290000334 2.890265241000028 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.827_-2.765.dat -nsteps 100000 -x Colvars/traj_-2.827_-2.765.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.170       15.170      100.0
                 (ns/day)    (hour/ns)
Performance:     1139.104        0.021

GROMACS reminds you: "It's Against the Rules" (Pulp Fiction)


# Integration error:        5.13

 -------------- BaysOpt loop, query 28 ---------------
going to run a simulation at -2.8274333879999514 -2.7646015350000686 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.827_2.702.dat -nsteps 100000 -x Colvars/traj_-2.827_2.702.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.788       14.788      100.0
                 (ns/day)    (hour/ns)
Performance:     1168.558        0.021

GROMACS reminds you: "You Dirty Switch, You're On Again" (The Breeders)


# Integration error:        4.93

 -------------- BaysOpt loop, query 29 ---------------
going to run a simulation at -2.8274333879999514 2.701769681999987 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.628_-0.314.dat -nsteps 100000 -x Colvars/traj_0.628_-0.314.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.237       15.237      100.0
                 (ns/day)    (hour/ns)
Performance:     1134.105        0.021

GROMACS reminds you: "I'd be Safe and Warm if I was in L.A." (The Mamas and the Papas)


# Integration error:        4.94

 -------------- BaysOpt loop, query 30 ---------------
going to run a simulation at 0.6283185309999932 -0.31415926500000063 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.000_2.765.dat -nsteps 100000 -x Colvars/traj_0.000_2.765.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.849       14.849      100.0
                 (ns/day)    (hour/ns)
Performance:     1163.705        0.021

GROMACS reminds you: "Take Dehydrated Water On Your Desert Trips" (Space Quest III)


# Integration error:        5.01

 -------------- BaysOpt loop, query 31 ---------------
going to run a simulation at 0.0 2.7646015350000686 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.571_0.440.dat -nsteps 100000 -x Colvars/traj_1.571_0.440.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.647       14.647      100.0
                 (ns/day)    (hour/ns)
Performance:     1179.749        0.020

GROMACS reminds you: "Nada e organico, e tudo programado" (Pitty)


# Integration error:        4.95

 -------------- BaysOpt loop, query 32 ---------------
going to run a simulation at 1.5707963269999845 0.43982297200000164 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.503_1.696.dat -nsteps 100000 -x Colvars/traj_0.503_1.696.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.146       15.146      100.0
                 (ns/day)    (hour/ns)
Performance:     1140.913        0.021

GROMACS reminds you: "I have not failed. I've just found 10,000 ways that won't work" (Thomas Alva Edison)


# Integration error:        4.53

 -------------- BaysOpt loop, query 33 ---------------
going to run a simulation at 0.5026548249999909 1.6964600329999944 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.571_2.450.dat -nsteps 100000 -x Colvars/traj_1.571_2.450.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.434       14.434      100.0
                 (ns/day)    (hour/ns)
Performance:     1197.194        0.020

GROMACS reminds you: "When you get right down to it, almost every explanation Man came up with for anything until about 1926 was stupid." (Dave Barry)


# Integration error:        4.20

 -------------- BaysOpt loop, query 34 ---------------
going to run a simulation at 1.5707963269999845 2.450442270000013 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.131_1.885.dat -nsteps 100000 -x Colvars/traj_-1.131_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.695       14.695      100.0
                 (ns/day)    (hour/ns)
Performance:     1175.916        0.020

GROMACS reminds you: "The most likely way for the world to be destroyed, most experts agree, is by accident. That's where we come in; we're computer professionals. We cause accidents." (Nathaniel Borenstein)


# Integration error:        3.47

 -------------- BaysOpt loop, query 35 ---------------
going to run a simulation at -1.130973355000006 1.8849555920000378 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.890_-0.063.dat -nsteps 100000 -x Colvars/traj_2.890_-0.063.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.681       14.681      100.0
                 (ns/day)    (hour/ns)
Performance:     1177.053        0.020

GROMACS reminds you: "In the Meantime, Take Care of Yourself aaand Eachother" (J. Springer)


# Integration error:        3.56

 -------------- BaysOpt loop, query 36 ---------------
going to run a simulation at 2.890265241000028 -0.06283185299999909 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.251_-2.953.dat -nsteps 100000 -x Colvars/traj_0.251_-2.953.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.865       14.865      100.0
                 (ns/day)    (hour/ns)
Performance:     1162.492        0.021

GROMACS reminds you: "Look at these, my work-strong arms" (P.J. Harvey)


# Integration error:        3.47

 -------------- BaysOpt loop, query 37 ---------------
going to run a simulation at 0.25132741199999636 -2.953097093999987 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.754_-1.885.dat -nsteps 100000 -x Colvars/traj_-0.754_-1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.971       14.971      100.0
                 (ns/day)    (hour/ns)
Performance:     1154.262        0.021

GROMACS reminds you: "Prior to 1965 there were none, and after 1965 there was a nun." (Sister Mary Kenneth Keller regarding women with PhDs in computer science)


# Integration error:        4.36

 -------------- BaysOpt loop, query 38 ---------------
going to run a simulation at -0.753982236999984 -1.8849555920000378 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.073_0.691.dat -nsteps 100000 -x Colvars/traj_-2.073_0.691.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.614       14.614      100.0
                 (ns/day)    (hour/ns)
Performance:     1182.464        0.020

GROMACS reminds you: "Garbage Collecting..." (GNU Emacs)


# Integration error:        3.92

 -------------- BaysOpt loop, query 39 ---------------
going to run a simulation at -2.0734511509999756 0.6911503840000147 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.634_-0.754.dat -nsteps 100000 -x Colvars/traj_-1.634_-0.754.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.702       14.702      100.0
                 (ns/day)    (hour/ns)
Performance:     1175.365        0.020

GROMACS reminds you: "Even the *healthy* people move in clouds of cigarette smoke, women straining polyester, men in raggedly cutoffs slathering mayonnaise on foot-long hot dogs. It's as if the hotel were hosting a conference on adult onset diabetes" (Jess Walter)


# Integration error:        3.84

 -------------- BaysOpt loop, query 40 ---------------
going to run a simulation at -1.6336281799999743 -0.753982236999984 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.890_-0.188.dat -nsteps 100000 -x Colvars/traj_-2.890_-0.188.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.816       14.816      100.0
                 (ns/day)    (hour/ns)
Performance:     1166.293        0.021

GROMACS reminds you: "For those who want some proof that physicists are human, the proof is in the idiocy of all the different units which they use for measuring energy." (Richard Feynman)


# Integration error:        3.85

 -------------- BaysOpt loop, query 41 ---------------
going to run a simulation at -2.890265241000028 -0.18849555899999812 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.765_1.885.dat -nsteps 100000 -x Colvars/traj_2.765_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.402       14.402      100.0
                 (ns/day)    (hour/ns)
Performance:     1199.860        0.020

GROMACS reminds you: "A Pretty Village Burning Makes a Pretty Fire" (David Sandstrom)


# Integration error:        3.88

 -------------- BaysOpt loop, query 42 ---------------
going to run a simulation at 2.7646015350000686 1.8849555920000378 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.696_-2.073.dat -nsteps 100000 -x Colvars/traj_-1.696_-2.073.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.280       15.280      100.0
                 (ns/day)    (hour/ns)
Performance:     1130.931        0.021

GROMACS reminds you: "Is it the invisible chemistry stuff?" (Frida Hyvonen)


# Integration error:        3.88

 -------------- BaysOpt loop, query 43 ---------------
going to run a simulation at -1.6964600329999944 -2.0734511509999756 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.827_-1.696.dat -nsteps 100000 -x Colvars/traj_2.827_-1.696.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.932       14.932      100.0
                 (ns/day)    (hour/ns)
Performance:     1157.268        0.021

GROMACS reminds you: "Aristotle maintained that women have fewer teeth than men; although he was twice married, it never occurred to him to verify this statement by examining his wives' mouths." (Bertrand Russell)


# Integration error:        3.82

 -------------- BaysOpt loop, query 44 ---------------
going to run a simulation at 2.8274333879999514 -1.6964600329999944 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.011_-2.890.dat -nsteps 100000 -x Colvars/traj_2.011_-2.890.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.178       15.178      100.0
                 (ns/day)    (hour/ns)
Performance:     1138.531        0.021

GROMACS reminds you: "Religion is a culture of faith; science is a culture of doubt." (Richard Feynman)


# Integration error:        3.73

 -------------- BaysOpt loop, query 45 ---------------
going to run a simulation at 2.0106192979999813 -2.890265241000028 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.885_1.382.dat -nsteps 100000 -x Colvars/traj_1.885_1.382.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.769       14.769      100.0
                 (ns/day)    (hour/ns)
Performance:     1170.004        0.021

GROMACS reminds you: "Sitting on a rooftop watching molecules collide" (A Camp)


# Integration error:        3.63

 -------------- BaysOpt loop, query 46 ---------------
going to run a simulation at 1.8849555920000378 1.3823007680000294 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.880_0.754.dat -nsteps 100000 -x Colvars/traj_-0.880_0.754.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.978       14.978      100.0
                 (ns/day)    (hour/ns)
Performance:     1153.704        0.021

GROMACS reminds you: "That Was Cool" (Beavis and Butthead)


# Integration error:        3.36

 -------------- BaysOpt loop, query 47 ---------------
going to run a simulation at -0.8796459430000121 0.753982236999984 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.325_1.696.dat -nsteps 100000 -x Colvars/traj_-2.325_1.696.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.807       14.807      100.0
                 (ns/day)    (hour/ns)
Performance:     1167.038        0.021

GROMACS reminds you: "For the first time we can now mechanically simulate the cognitive process. We can make studies in artificial intelligence. Beyond that, this mechanism can be used to assist humans in learning. As we are going to have more mature students in greater numbers as time goes on, this type of teaching will probably be increasingly important." (Sister Mary Kenneth Keller)


# Integration error:        3.42

 -------------- BaysOpt loop, query 48 ---------------
going to run a simulation at -2.3247785640000505 1.6964600329999944 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.885_2.827.dat -nsteps 100000 -x Colvars/traj_-1.885_2.827.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.098       15.098      100.0
                 (ns/day)    (hour/ns)
Performance:     1144.566        0.021

GROMACS reminds you: "A Pretty Village Burning Makes a Pretty Fire" (David Sandstrom)


# Integration error:        3.43

 -------------- BaysOpt loop, query 49 ---------------
going to run a simulation at -1.8849555920000378 2.8274333879999514 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.822_-2.953.dat -nsteps 100000 -x Colvars/traj_-1.822_-2.953.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.741       14.741      100.0
                 (ns/day)    (hour/ns)
Performance:     1172.239        0.020

GROMACS reminds you: "Apologies to the astrophysics student I met at a party years ago. When you told me how many hours a day you used 4chan and how much you love it, I gave you a funny look and walked away. Now, a decade later, I realize you were talking about Fortran." (Anonymous)


# Integration error:        3.37

 -------------- BaysOpt loop, query 50 ---------------
going to run a simulation at -1.8221237390000142 -2.953097093999987 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.765_-1.948.dat -nsteps 100000 -x Colvars/traj_-2.765_-1.948.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.731       14.731      100.0
                 (ns/day)    (hour/ns)
Performance:     1173.075        0.020

GROMACS reminds you: "What do you want out of life?" (Jack Kerouac, On The Road)


# Integration error:        3.42

 -------------- BaysOpt loop, query 51 ---------------
going to run a simulation at -2.7646015350000686 -1.9477874449999604 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.691_-0.503.dat -nsteps 100000 -x Colvars/traj_-0.691_-0.503.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.038       15.038      100.0
                 (ns/day)    (hour/ns)
Performance:     1149.120        0.021

GROMACS reminds you: "Praise those of your critics for whom nothing is up to standard." (Dag Hammarskjold)


# Integration error:        3.41

 -------------- BaysOpt loop, query 52 ---------------
going to run a simulation at -0.6911503840000147 -0.5026548249999909 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.314_0.754.dat -nsteps 100000 -x Colvars/traj_0.314_0.754.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.823       14.823      100.0
                 (ns/day)    (hour/ns)
Performance:     1165.734        0.021

GROMACS reminds you: "You ONLY have to do the coding ..." (Anton Jansen, to core developer, on implementing new features)


# Integration error:        3.36

 -------------- BaysOpt loop, query 53 ---------------
going to run a simulation at 0.31415926500000063 0.753982236999984 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.188_-1.194.dat -nsteps 100000 -x Colvars/traj_0.188_-1.194.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.594       14.594      100.0
                 (ns/day)    (hour/ns)
Performance:     1184.029        0.020

GROMACS reminds you: "It's easy to remember: a half a kT is equal to five fourths of a kJ/mol." (Anders Gabrielsson)


# Integration error:        3.14

 -------------- BaysOpt loop, query 54 ---------------
going to run a simulation at 0.18849555899999812 -1.1938052079999777 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.571_-1.634.dat -nsteps 100000 -x Colvars/traj_1.571_-1.634.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.000       15.000      100.0
                 (ns/day)    (hour/ns)
Performance:     1151.973        0.021

GROMACS reminds you: "Measuring programming progress by lines of code is like measuring aircraft building progress by weight." (Bill Gates)


# Integration error:        2.99

 -------------- BaysOpt loop, query 55 ---------------
going to run a simulation at 1.5707963269999845 -1.6336281799999743 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.571_-0.251.dat -nsteps 100000 -x Colvars/traj_1.571_-0.251.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.774       14.774      100.0
                 (ns/day)    (hour/ns)
Performance:     1169.636        0.021

GROMACS reminds you: "error: too many template-parameter-lists" (g++)


# Integration error:        2.99

 -------------- BaysOpt loop, query 56 ---------------
going to run a simulation at 1.5707963269999845 -0.25132741199999636 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.953_1.948.dat -nsteps 100000 -x Colvars/traj_-2.953_1.948.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.073       15.073      100.0
                 (ns/day)    (hour/ns)
Performance:     1146.455        0.021

GROMACS reminds you: "Take Your Medications and Preparations and Ram It Up Your Snout" (F. Zappa)


# Integration error:        3.01

 -------------- BaysOpt loop, query 57 ---------------
going to run a simulation at -2.953097093999987 1.9477874449999604 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.827_0.565.dat -nsteps 100000 -x Colvars/traj_-2.827_0.565.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.830       14.830      100.0
                 (ns/day)    (hour/ns)
Performance:     1165.228        0.021

GROMACS reminds you: "BioBeat is Not Available In Regular Shops" (P.J. Meulenhoff)


# Integration error:        2.95

 -------------- BaysOpt loop, query 58 ---------------
going to run a simulation at -2.8274333879999514 0.5654866780000047 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.325_-0.754.dat -nsteps 100000 -x Colvars/traj_-2.325_-0.754.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.969       14.969      100.0
                 (ns/day)    (hour/ns)
Performance:     1154.400        0.021

GROMACS reminds you: "Like other defaulters, I like to lay half the blame on ill-fortune and adverse circumstances" (Mr. Rochester in Jane Eyre by Charlotte Bronte)


# Integration error:        2.92

 -------------- BaysOpt loop, query 59 ---------------
going to run a simulation at -2.3247785640000505 -0.753982236999984 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.765_0.691.dat -nsteps 100000 -x Colvars/traj_2.765_0.691.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       16.353       16.353      100.0
                 (ns/day)    (hour/ns)
Performance:     1056.692        0.023

GROMACS reminds you: "It was something to at least have a choice of nightmares" (Joseph Conrad)


# Integration error:        2.98

 -------------- BaysOpt loop, query 60 ---------------
going to run a simulation at 2.7646015350000686 0.6911503840000147 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.565_-2.827.dat -nsteps 100000 -x Colvars/traj_-0.565_-2.827.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.898       14.898      100.0
                 (ns/day)    (hour/ns)
Performance:     1159.902        0.021

GROMACS reminds you: "They're Red Hot" (Red Hot Chili Peppers)


# Integration error:        2.95

 -------------- BaysOpt loop, query 61 ---------------
going to run a simulation at -0.5654866780000047 -2.8274333879999514 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.628_-2.513.dat -nsteps 100000 -x Colvars/traj_0.628_-2.513.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.645       14.645      100.0
                 (ns/day)    (hour/ns)
Performance:     1179.929        0.020

GROMACS reminds you: "Correctomundo" (Pulp Fiction)


# Integration error:        1.98

 -------------- BaysOpt loop, query 62 ---------------
going to run a simulation at 0.6283185309999932 -2.513274122999977 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.691_2.827.dat -nsteps 100000 -x Colvars/traj_-0.691_2.827.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.712       14.712      100.0
                 (ns/day)    (hour/ns)
Performance:     1174.548        0.020

GROMACS reminds you: "This Puke Stinks Like Beer" (LIVE)


# Integration error:        1.91

 -------------- BaysOpt loop, query 63 ---------------
going to run a simulation at -0.6911503840000147 2.8274333879999514 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.885_2.890.dat -nsteps 100000 -x Colvars/traj_1.885_2.890.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.775       14.775      100.0
                 (ns/day)    (hour/ns)
Performance:     1169.552        0.021

GROMACS reminds you: "Art For Arts Sake, Money For Gods Sake" (10 CC)


# Integration error:        1.91

 -------------- BaysOpt loop, query 64 ---------------
going to run a simulation at 1.8849555920000378 2.890265241000028 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.759_2.011.dat -nsteps 100000 -x Colvars/traj_-1.759_2.011.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.619       14.619      100.0
                 (ns/day)    (hour/ns)
Performance:     1182.054        0.020

GROMACS reminds you: "It all works because Avogadro's number is closer to infinity than to 10." (Ralph Baierlein)


# Integration error:        1.69

 -------------- BaysOpt loop, query 65 ---------------
going to run a simulation at -1.7592918860000242 2.0106192979999813 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.126_-0.188.dat -nsteps 100000 -x Colvars/traj_0.126_-0.188.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.676       14.676      100.0
                 (ns/day)    (hour/ns)
Performance:     1177.452        0.020

GROMACS reminds you: "This isn't right. This isn't even wrong." (Wolfgang Pauli)


# Integration error:        1.66

 -------------- BaysOpt loop, query 66 ---------------
going to run a simulation at 0.12566370599999818 -0.18849555899999812 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.450_-0.503.dat -nsteps 100000 -x Colvars/traj_2.450_-0.503.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.536       14.536      100.0
                 (ns/day)    (hour/ns)
Performance:     1188.758        0.020

GROMACS reminds you: "On the East coast, a purple patch, to show where the jolly pioneers of progress drink the jolly lager-beer" (Joseph Conrad)


# Integration error:        1.69

 -------------- BaysOpt loop, query 67 ---------------
going to run a simulation at 2.450442270000013 -0.5026548249999909 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.885_0.126.dat -nsteps 100000 -x Colvars/traj_-1.885_0.126.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.589       14.589      100.0
                 (ns/day)    (hour/ns)
Performance:     1184.435        0.020

GROMACS reminds you: "Sincerity is the key to success. Once you can fake that you've got it made." (Groucho Marx)


# Integration error:        1.72

 -------------- BaysOpt loop, query 68 ---------------
going to run a simulation at -1.8849555920000378 0.12566370599999818 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.691_1.634.dat -nsteps 100000 -x Colvars/traj_-0.691_1.634.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.790       14.790      100.0
                 (ns/day)    (hour/ns)
Performance:     1168.367        0.021

GROMACS reminds you: "Lunatics On Pogo Sticks" (Red Hot Chili Peppers)


# Integration error:        1.73

 -------------- BaysOpt loop, query 69 ---------------
going to run a simulation at -0.6911503840000147 1.6336281799999743 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.503_2.450.dat -nsteps 100000 -x Colvars/traj_0.503_2.450.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.574       14.574      100.0
                 (ns/day)    (hour/ns)
Performance:     1185.647        0.020

GROMACS reminds you: "The Lord of the Rings can be confusing to follow because many of the bad minions look and sound familiar; that's why Tolkien gave them each an ORCid." (Caroline Bartman)


# Integration error:        1.72

 -------------- BaysOpt loop, query 70 ---------------
going to run a simulation at 0.5026548249999909 2.450442270000013 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.063_-1.696.dat -nsteps 100000 -x Colvars/traj_0.063_-1.696.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.230       14.230      100.0
                 (ns/day)    (hour/ns)
Performance:     1214.342        0.020

GROMACS reminds you: "I don't know how many of you have ever met Dijkstra, but you probably know that arrogance in computer science is measured in nano-Dijkstras." (Alan Kay)


# Integration error:        2.45

 -------------- BaysOpt loop, query 71 ---------------
going to run a simulation at 0.06283185299999909 -1.6964600329999944 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.073_-1.257.dat -nsteps 100000 -x Colvars/traj_2.073_-1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.430       15.430      100.0
                 (ns/day)    (hour/ns)
Performance:     1119.933        0.021

GROMACS reminds you: "The easiest way to scale well is to have bad single-core performance" (Blind Freddie)


# Integration error:        2.44

 -------------- BaysOpt loop, query 72 ---------------
going to run a simulation at 2.0734511509999756 -1.2566370609999933 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.953_2.576.dat -nsteps 100000 -x Colvars/traj_2.953_2.576.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.728       14.728      100.0
                 (ns/day)    (hour/ns)
Performance:     1173.281        0.020

GROMACS reminds you: "Our hands are tied by physics." (Christian Blau)


# Integration error:        2.47

 -------------- BaysOpt loop, query 73 ---------------
going to run a simulation at 2.953097093999987 2.576105975999961 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.571_2.073.dat -nsteps 100000 -x Colvars/traj_1.571_2.073.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.933       14.933      100.0
                 (ns/day)    (hour/ns)
Performance:     1157.159        0.021

GROMACS reminds you: "Jesus Built My Hotrod" (Ministry)


# Integration error:        2.45

 -------------- BaysOpt loop, query 74 ---------------
going to run a simulation at 1.5707963269999845 2.0734511509999756 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.068_1.068.dat -nsteps 100000 -x Colvars/traj_1.068_1.068.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.860       14.860      100.0
                 (ns/day)    (hour/ns)
Performance:     1162.863        0.021

GROMACS reminds you: "A computer without COBOL and FORTRAN is like a piece of chocolate cake without ketchup or mustard." (Unix fortune program)


# Integration error:        2.46

 -------------- BaysOpt loop, query 75 ---------------
going to run a simulation at 1.0681415019999982 1.0681415019999982 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.011_0.754.dat -nsteps 100000 -x Colvars/traj_2.011_0.754.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.721       14.721      100.0
                 (ns/day)    (hour/ns)
Performance:     1173.846        0.020

GROMACS reminds you: "A tidy laboratory means a lazy chemist." (J.J. Berzelius)


# Integration error:        2.45

 -------------- BaysOpt loop, query 76 ---------------
going to run a simulation at 2.0106192979999813 0.753982236999984 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.011_-1.571.dat -nsteps 100000 -x Colvars/traj_-2.011_-1.571.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.790       14.790      100.0
                 (ns/day)    (hour/ns)
Performance:     1168.348        0.021

GROMACS reminds you: "My Ass May Be Dumb, But I Ain't No Dumbass." (Jackie Brown)


# Integration error:        2.43

 -------------- BaysOpt loop, query 77 ---------------
going to run a simulation at -2.0106192979999813 -1.5707963269999845 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.890_-0.942.dat -nsteps 100000 -x Colvars/traj_-2.890_-0.942.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.682       14.682      100.0
                 (ns/day)    (hour/ns)
Performance:     1176.975        0.020

GROMACS reminds you: "I Wonder, Should I Get Up..." (J. Lennon)


# Integration error:        2.42

 -------------- BaysOpt loop, query 78 ---------------
going to run a simulation at -2.890265241000028 -0.9424777960000189 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.880_-1.131.dat -nsteps 100000 -x Colvars/traj_-0.880_-1.131.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.662       14.662      100.0
                 (ns/day)    (hour/ns)
Performance:     1178.601        0.020

GROMACS reminds you: "Boom Boom Boom Boom, I Want You in My Room" (Venga Boys)


# Integration error:        2.41

 -------------- BaysOpt loop, query 79 ---------------
going to run a simulation at -0.8796459430000121 -1.130973355000006 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.005_0.063.dat -nsteps 100000 -x Colvars/traj_1.005_0.063.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.636       14.636      100.0
                 (ns/day)    (hour/ns)
Performance:     1180.622        0.020

GROMACS reminds you: "Consistently separating words by spaces became a general custom about the tenth century A.D., and lasted until about 1957, when FORTRAN abandoned the practice." (Sun FORTRAN Reference Manual)


# Integration error:        2.40

 -------------- BaysOpt loop, query 80 ---------------
going to run a simulation at 1.0053096489999906 0.06283185299999909 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.634_-2.576.dat -nsteps 100000 -x Colvars/traj_1.634_-2.576.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.114       15.114      100.0
                 (ns/day)    (hour/ns)
Performance:     1143.320        0.021

GROMACS reminds you: "I wanted to make a clever chemistry joke, but the best ones Argon." (39.948)


# Integration error:        2.39

 -------------- BaysOpt loop, query 81 ---------------
going to run a simulation at 1.6336281799999743 -2.576105975999961 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.136_-2.702.dat -nsteps 100000 -x Colvars/traj_-2.136_-2.702.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.667       14.667      100.0
                 (ns/day)    (hour/ns)
Performance:     1178.156        0.020

GROMACS reminds you: "Does All This Money Really Have To Go To Charity ?" (Rick)


# Integration error:        2.40

 -------------- BaysOpt loop, query 82 ---------------
going to run a simulation at -2.1362830039999965 -2.701769681999987 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.126_-0.691.dat -nsteps 100000 -x Colvars/traj_0.126_-0.691.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.703       14.703      100.0
                 (ns/day)    (hour/ns)
Performance:     1175.259        0.020

GROMACS reminds you: "Science is organized knowledge. Wisdom is organized life." (Immanuel Kant)


# Integration error:        2.38

 -------------- BaysOpt loop, query 83 ---------------
going to run a simulation at 0.12566370599999818 -0.6911503840000147 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.702_1.445.dat -nsteps 100000 -x Colvars/traj_2.702_1.445.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.855       14.855      100.0
                 (ns/day)    (hour/ns)
Performance:     1163.243        0.021

GROMACS reminds you: "C has the power of assembly language and the convenience of... assembly language." (Dennis Ritchie)


# Integration error:        2.35

 -------------- BaysOpt loop, query 84 ---------------
going to run a simulation at 2.701769681999987 1.4451326210000188 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.890_-0.880.dat -nsteps 100000 -x Colvars/traj_2.890_-0.880.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.410       14.410      100.0
                 (ns/day)    (hour/ns)
Performance:     1199.190        0.020

GROMACS reminds you: "I Ripped the Cord Right Out Of the Phone" (Capt. Beefheart)


# Integration error:        2.36

 -------------- BaysOpt loop, query 85 ---------------
going to run a simulation at 2.890265241000028 -0.8796459430000121 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.450_2.890.dat -nsteps 100000 -x Colvars/traj_-2.450_2.890.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.550       14.550      100.0
                 (ns/day)    (hour/ns)
Performance:     1187.602        0.020

GROMACS reminds you: "Don't Push Me, Cause I'm Close to the Edge" (Tricky)


# Integration error:        2.33

 -------------- BaysOpt loop, query 86 ---------------
going to run a simulation at -2.450442270000013 2.890265241000028 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.382_-2.450.dat -nsteps 100000 -x Colvars/traj_-1.382_-2.450.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.676       14.676      100.0
                 (ns/day)    (hour/ns)
Performance:     1177.428        0.020

GROMACS reminds you: "Aristotle maintained that women have fewer teeth than men; although he was twice married, it never occurred to him to verify this statement by examining his wives' mouths." (Bertrand Russell)


# Integration error:        2.33

 -------------- BaysOpt loop, query 87 ---------------
going to run a simulation at -1.3823007680000294 -2.450442270000013 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.131_2.890.dat -nsteps 100000 -x Colvars/traj_1.131_2.890.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.017       15.017      100.0
                 (ns/day)    (hour/ns)
Performance:     1150.675        0.021

GROMACS reminds you: "There's no way you can rely on an experiment" (Gerrit Groenhof)


# Integration error:        2.28

 -------------- BaysOpt loop, query 88 ---------------
going to run a simulation at 1.130973355000006 2.890265241000028 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.068_1.948.dat -nsteps 100000 -x Colvars/traj_1.068_1.948.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.520       14.521      100.0
                 (ns/day)    (hour/ns)
Performance:     1190.052        0.020

GROMACS reminds you: "Studying expands knowledge. Knowledge is power. Power corrupts. Corruption is a crime. Crime doesn't pay." (Anonymous)


# Integration error:        2.23

 -------------- BaysOpt loop, query 89 ---------------
going to run a simulation at 1.0681415019999982 1.9477874449999604 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.880_0.251.dat -nsteps 100000 -x Colvars/traj_-0.880_0.251.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.052       15.053      100.0
                 (ns/day)    (hour/ns)
Performance:     1147.993        0.021

GROMACS reminds you: "A programmer's spouse says 'While you're at the grocery store, buy some eggs.' The programmer never comes back." (Anonymous)


# Integration error:        2.29

 -------------- BaysOpt loop, query 90 ---------------
going to run a simulation at -0.8796459430000121 0.25132741199999636 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.126_0.503.dat -nsteps 100000 -x Colvars/traj_-0.126_0.503.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.107       15.107      100.0
                 (ns/day)    (hour/ns)
Performance:     1143.873        0.021

GROMACS reminds you: "Never, I said never, compare with experiment" (Magnus Bergh)


# Integration error:        2.34

 -------------- BaysOpt loop, query 91 ---------------
going to run a simulation at -0.12566370599999818 0.5026548249999909 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_0.251_1.382.dat -nsteps 100000 -x Colvars/traj_0.251_1.382.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.738       14.738      100.0
                 (ns/day)    (hour/ns)
Performance:     1172.518        0.020

GROMACS reminds you: "A Pretty Village Burning Makes a Pretty Fire" (David Sandstrom)


# Integration error:        2.34

 -------------- BaysOpt loop, query 92 ---------------
going to run a simulation at 0.25132741199999636 1.3823007680000294 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.450_-1.885.dat -nsteps 100000 -x Colvars/traj_2.450_-1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.800       14.800      100.0
                 (ns/day)    (hour/ns)
Performance:     1167.581        0.021

GROMACS reminds you: "Apologies to the astrophysics student I met at a party years ago. When you told me how many hours a day you used 4chan and how much you love it, I gave you a funny look and walked away. Now, a decade later, I realize you were talking about Fortran." (Anonymous)


# Integration error:        2.30

 -------------- BaysOpt loop, query 93 ---------------
going to run a simulation at 2.450442270000013 -1.8849555920000378 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_2.890_-2.953.dat -nsteps 100000 -x Colvars/traj_2.890_-2.953.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       15.001       15.001      100.0
                 (ns/day)    (hour/ns)
Performance:     1151.924        0.021

GROMACS reminds you: "A professor is one who talks in someone else's sleep." (W.H. Auden)


# Integration error:        2.30

 -------------- BaysOpt loop, query 94 ---------------
going to run a simulation at 2.890265241000028 -2.953097093999987 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-2.073_1.257.dat -nsteps 100000 -x Colvars/traj_-2.073_1.257.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.815       14.815      100.0
                 (ns/day)    (hour/ns)
Performance:     1166.414        0.021

GROMACS reminds you: "It is disconcerting to reflect on the number of students we have flunked in chemistry for not knowing what we later found to be untrue." (Robert L. Weber)


# Integration error:        2.25

 -------------- BaysOpt loop, query 95 ---------------
going to run a simulation at -2.0734511509999756 1.2566370609999933 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.445_0.691.dat -nsteps 100000 -x Colvars/traj_-1.445_0.691.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.845       14.845      100.0
                 (ns/day)    (hour/ns)
Performance:     1164.025        0.021

GROMACS reminds you: "I Am a Poor Lonesome Cowboy" (Lucky Luke)


# Integration error:        2.25

 -------------- BaysOpt loop, query 96 ---------------
going to run a simulation at -1.4451326210000188 0.6911503840000147 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-1.319_-0.377.dat -nsteps 100000 -x Colvars/traj_-1.319_-0.377.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.648       14.648      100.0
                 (ns/day)    (hour/ns)
Performance:     1179.696        0.020

GROMACS reminds you: "BioBeat is Not Available In Regular Shops" (P.J. Meulenhoff)


# Integration error:        2.20

 -------------- BaysOpt loop, query 97 ---------------
going to run a simulation at -1.3194689150000116 -0.37699111799999624 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_-0.880_1.194.dat -nsteps 100000 -x Colvars/traj_-0.880_1.194.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.722       14.722      100.0
                 (ns/day)    (hour/ns)
Performance:     1173.768        0.020

GROMACS reminds you: "It's Coming Right For Us !" (South Park)


# Integration error:        2.20

 -------------- BaysOpt loop, query 98 ---------------
going to run a simulation at -0.8796459430000121 1.1938052079999777 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.948_1.885.dat -nsteps 100000 -x Colvars/traj_1.948_1.885.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.958       14.958      100.0
                 (ns/day)    (hour/ns)
Performance:     1155.233        0.021

GROMACS reminds you: "Kick the Dog and You Will Die" (Magnapop)


# Integration error:        2.17

 -------------- BaysOpt loop, query 99 ---------------
going to run a simulation at 1.9477874449999604 1.8849555920000378 
# Integrating             - Simpson's rule + Real Space Grid Mini                  :-) GROMACS - gmx mdrun, 2023-plumed_2.9.1 (-:

Executable:   /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc/bin/gmx_mpi
Data prefix:  /opt/sw-fnwi/hims/2024/spack/opt/spack/linux-almalinux8-zen4/gcc-12.4.0/gromacs-2023-clqvxslnp5qwbuc5vyvniia2gfyvurpc
Working dir:  /local_scratch/ekempke/131128
Command line:
  gmx_mpi mdrun -s md.tpr -plumed Colvars/plumed_1.005_-2.953.dat -nsteps 100000 -x Colvars/traj_1.005_-2.953.xtc


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD: AVX_512, but for this host/run AVX2_256 might be better (see
log).
Reading file md.tpr, VERSION 2022.5-plumed_2.9.3 (single precision)
Note: file tpx version 127, software tpx version 129
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100000 steps, 200 ps

comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj.trr to ./#traj.trr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generated by trjconv : Generated by trjconv : Alanine in vacuum in water t= 4500.00000'
100000 steps,    200.0 ps.

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:       14.717       14.717      100.0
                 (ns/day)    (hour/ns)
Performance:     1174.149        0.020

GROMACS reminds you: "Those who cannot remember the past are condemned to compute it." (Steve Pinker)


# Integration error:        2.15

 -------------- BaysOpt loop, query 100 ---------------
going to run a simulation at 1.0053096489999906 -2.953097093999987 
# Integrating             - Simpson's rule + Real Space Grid Mini 
# Integration error:        2.08

Results saved to AD_0.2_ns_Matern52_ls_0.75_w_0.1_n0.0_acq_IVR_q_100_adipep.txt
